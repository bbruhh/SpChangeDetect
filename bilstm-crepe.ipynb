{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crepe\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from scipy.io import wavfile\n",
    "\n",
    "root_dir = \"./amicorpus/*/audio/\"\n",
    "ami_corpus = glob.glob(os.path.join(root_dir, '*wav'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in ami_corpus:\n",
    "    try:\n",
    "        sr, audio = wavfile.read(file)\n",
    "        filename = (file.split(\"/\")[-1]).split(\".\")[0]\n",
    "        print (filename)\n",
    "        time, frequency, confidence, activation = crepe.predict(audio, sr, viterbi=True, step_size=100)\n",
    "        np.save(\"./crepe_ami_storage/\" + filename + \"Mix-Headset_freq\", frequency)\n",
    "        np.save(\"./crepe_ami_storage/\" + filename + \"Mix-Headset_conf\", confidence)\n",
    "\n",
    "    except:\n",
    "        print (\"pass\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, audio = wavfile.read(\"TV_bubi6_2017-04-11_gek.wav\")\n",
    "time, frequency, confidence, activation = crepe.predict(audio, sr, viterbi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.rcParams['figure.figsize'] = (50.8, 10.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(confidence[8000:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(frequency[790:930])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def create_data_for_supervised(root_dir, hop, win_len, from_ep=0, to_ep=0, boost_for_imbalance=False, how_much_boost=6, balance=False,\n",
    "                              overlapping=False,\n",
    "                              fuzzy_label=False):\n",
    "    \n",
    "    ## If we have numpy array in the folder, we will create input and output array via this function. Its arguments:\n",
    "    # root_dir: The folder which stores numpy array.\n",
    "    # hop: Hop length (we need it for Librosa)\n",
    "    # win_len: Window length (we need it for Librosa)\n",
    "    # from_ep: Location of first file which will be loaded into array\n",
    "    # to_ep: Location of last file which will be loaded into array\n",
    "    # boost_for_imbalance: If it is true, the number of positive labels is\n",
    "    # increased artificially by labeling as positive every frame in the\n",
    "    # direct  neighborhood  of  the  manually  annotated  change  point\n",
    "    # and this number will be determined by how_much_boost parameter\n",
    "    # balance: if it is true, we will discard some frame which are at the \n",
    "    # middle of single speaker segment.\n",
    "    \n",
    "    \n",
    "    all_audio_paths = glob.glob(os.path.join(root_dir, '*wav'))\n",
    "    matrix_of_all_audio = []\n",
    "    \n",
    "    output_all_array = []\n",
    "    num = 0\n",
    "    \n",
    "    for single_audio_path in all_audio_paths:\n",
    "        num += 1\n",
    "        if ((num >= from_ep) and (num < to_ep)):\n",
    "            \n",
    "            end_time_array_second = []\n",
    "\n",
    "            filename = (single_audio_path.split(\"/\")[-1]).split(\".\")[0]\n",
    "            print (filename)\n",
    "\n",
    "            try:\n",
    "   \n",
    "                matrix_of_single_audio = np.load(\"/home/herdogan/Desktop/SpChangeDetect/crepe_ami_storage/\" + filename + \".npy\")\n",
    "\n",
    "                print (matrix_of_single_audio.shape)\n",
    "                    \n",
    "                array_of_single_audio = np.expand_dims(matrix_of_single_audio, axis=1)\n",
    "                \n",
    "                if (matrix_of_single_audio is not None):\n",
    "\n",
    "                    matrix_of_all_audio.extend(array_of_single_audio)\n",
    "                    print (single_audio_path + \" is done.\")\n",
    "\n",
    "                    main_set = \"./txt_ami_full/\" + filename + \"_full_time.txt\"# FILENAME PATH for TXT\n",
    "\n",
    "                    with open(main_set) as f:\n",
    "                        content = f.readlines()\n",
    "\n",
    "                    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "\n",
    "\n",
    "                    # need to open text file\n",
    "                    # after that, point the end point of speaker\n",
    "                    # add 1 to point of speaker, add 0 otherwise\n",
    "                    # time is in second format at the txt file\n",
    "                    content = [x.strip() for x in content] \n",
    "\n",
    "                    for single_line in content:\n",
    "\n",
    "                        end_time_array_second.append(single_line)\n",
    "\n",
    "                        # we use following method to get milisecond version\n",
    "                        # float(win_len + ((offset+100) * hop)) \n",
    "                        # we need to inversion of that\n",
    "                    # print (end_time_array_second)\n",
    "\n",
    "                    output_array = np.zeros(matrix_of_single_audio.shape[0])\n",
    "\n",
    "                    for end_time in end_time_array_second:\n",
    "                        end_time_ms = float(end_time)*1000\n",
    "                        \n",
    "                        which_start_hop = (end_time_ms-win_len)/hop # now we know, milisecond version of change\n",
    "                                                    # which is located after which_hop paramater\n",
    "                                                    # add 2 and round to up\n",
    "                        which_end_hop = end_time_ms/hop # round to up\n",
    "\n",
    "                        start_location = math.ceil(which_start_hop + 1)\n",
    "                        end_location = math.ceil(which_end_hop)\n",
    "\n",
    "                        # print (\"s:\", start_location)\n",
    "                        # print (\"e:\", end_location)\n",
    "                        if (boost_for_imbalance==False):\n",
    "                            output_array[start_location:end_location+1] = 1.0\n",
    "\n",
    "                        else:\n",
    "                            if (fuzzy_label==False):\n",
    "                                output_array[start_location-how_much_boost:end_location+1+how_much_boost] = 1.0\n",
    "                            else:\n",
    "                                output_array[start_location:end_location+1] = 1.0\n",
    "                                for ix_label in range(1, how_much_boost):\n",
    "                                    output_array[start_location-ix_label] = 1.0 - (float(ix_label)/how_much_boost)\n",
    "                                    output_array[end_location+1+ix_label] = 1.0 - (float(ix_label)/how_much_boost)\n",
    "                    output_all_array.extend(output_array)\n",
    "            except:\n",
    "                print (\"Pass this file...\")\n",
    "                pass\n",
    "            # print (output_array)\n",
    "            # print (output_array.mean())\n",
    "            # ar = np.arange(matrix_of_single_audio.shape[1]) # just as an example array\n",
    "            # pp.plot(ar, output_array, 'x')\n",
    "            # pp.show()\n",
    "    \n",
    "    # if (overlapping):\n",
    "        ### IMPLEMENT IT\n",
    "            \n",
    "    audio_array = np.asarray(matrix_of_all_audio)\n",
    "    \n",
    "    input_array = audio_array\n",
    "    \n",
    "    output_all_array = np.asarray(output_all_array)\n",
    "    output_all_array = np.expand_dims(output_all_array, axis=1)\n",
    "    \n",
    "    if (balance == True):\n",
    "            loc_zeros = np.where(output_all_array == 0)[0]\n",
    "            list_cons = [list(group) for group in mit.consecutive_groups(loc_zeros)]\n",
    "            for single_list_con in list_cons:\n",
    "                if (len(single_list_con) > 80):\n",
    "                    first_zero_loc = single_list_con[0]\n",
    "                    last_zero_loc = single_list_con[-1]\n",
    "                    output_all_array[first_zero_loc+20:last_zero_loc-20] = 2\n",
    "            loc_twos = np.where(output_all_array == 2)[0]\n",
    "            list_cons = [list(group) for group in mit.consecutive_groups(loc_twos)]\n",
    "            output_all_array = output_all_array.squeeze(axis=1)\n",
    "            # print (output_all_array.shape)\n",
    "            input_array = input_array[output_all_array != 2, :]\n",
    "            # [idx==0,:]\n",
    "            output_all_array = output_all_array[output_all_array != 2]   \n",
    "            output_all_array = np.expand_dims(output_all_array, axis=1)\n",
    "            \n",
    "    \n",
    "    print(\"inputs shape: \", input_array.shape)\n",
    "\n",
    "    print(\"outputs shape: \", output_all_array.shape)\n",
    "\n",
    "    return (input_array, output_all_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "frame_shape = (800, 1)\n",
    "\n",
    "## Network Architecture\n",
    "# https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "input_frame = keras.Input(frame_shape, name='main_input')\n",
    "\n",
    "conv1 = layers.Conv1D(800, 9, padding=\"same\")(input_frame)\n",
    "conv1 = layers.LeakyReLU()(conv1)\n",
    "conv1_BN = layers.BatchNormalization()(conv1)\n",
    "conv1_drop = layers.Dropout(0.5)(conv1_BN)\n",
    "\n",
    "#conv2 = layers.Conv1D(800, 9, padding=\"same\")(conv1_drop)\n",
    "#conv2 = layers.LeakyReLU()(conv2)\n",
    "#conv2_BN = layers.BatchNormalization()(conv2)\n",
    "#conv2_drop = layers.Dropout(0.3)(conv2_BN)\n",
    "\n",
    "\n",
    "bidirectional_1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True, kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))(conv1_drop)\n",
    "bidirectional_1 = layers.LeakyReLU()(bidirectional_1)\n",
    "bidirectional_1_BN = layers.BatchNormalization()(bidirectional_1)\n",
    "bidirectional_1_drop = layers.Dropout(0.5)(bidirectional_1_BN)\n",
    "\n",
    "bidirectional_2 = layers.Bidirectional(layers.LSTM(64, return_sequences=True,  kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))(bidirectional_1_drop)\n",
    "bidirectional_2 = layers.LeakyReLU()(bidirectional_2)\n",
    "bidirectional_2_BN = layers.BatchNormalization()(bidirectional_2)\n",
    "bidirectional_2_drop = layers.Dropout(0.5)(bidirectional_2_BN)\n",
    "\n",
    "bidirectional_3 = layers.Bidirectional(layers.LSTM(36, return_sequences=True,  kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))(bidirectional_2_drop)\n",
    "bidirectional_3 = layers.LeakyReLU()(bidirectional_3)\n",
    "bidirectional_3_BN = layers.BatchNormalization()(bidirectional_3)\n",
    "bidirectional_3_drop = layers.Dropout(0.5)(bidirectional_3_BN)\n",
    "\n",
    "\n",
    "tdistributed_1 = layers.TimeDistributed(layers.Dense(40, kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))(bidirectional_3_drop)\n",
    "tdistributed_1 = layers.LeakyReLU()(tdistributed_1)\n",
    "tdistributed_1_BN = layers.BatchNormalization()(tdistributed_1)\n",
    "tdistributed_1_drop = layers.Dropout(0.5)(tdistributed_1_BN)\n",
    "\n",
    "tdistributed_2 = layers.TimeDistributed(layers.Dense(10, kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))(tdistributed_1_drop)\n",
    "tdistributed_2 = layers.LeakyReLU()(tdistributed_2)\n",
    "tdistributed_2_BN = layers.BatchNormalization()(tdistributed_2)\n",
    "tdistributed_2_drop = layers.Dropout(0.5)(tdistributed_2_BN)\n",
    "\n",
    "\n",
    "tdistributed_3 = layers.TimeDistributed(layers.Dense(1, activation='sigmoid', kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))(tdistributed_2_drop)\n",
    "\n",
    "\n",
    "# WE DO NOT NEED IT FOR TRAINING. SO DISCARD.\n",
    "## Source: https://stackoverflow.com/questions/37743574/hard-limiting-threshold-activation-function-in-tensorflow\n",
    "def step_activation(x):\n",
    "    threshold = 0.4\n",
    "    cond = tf.less(x, tf.fill(value=threshold, dims=tf.shape(x)))\n",
    "    out = tf.where(cond, tf.zeros(tf.shape(x)), tf.ones(tf.shape(x)))\n",
    "\n",
    "    return out\n",
    "\n",
    "# https://stackoverflow.com/questions/47034692/keras-set-output-of-intermediate-layer-to-0-or-1-based-on-threshold\n",
    "\n",
    "step_activation = layers.Dense(1, activation=step_activation, name='threshold_activation')(tdistributed_3)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(input_frame, tdistributed_3)\n",
    "\n",
    "rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.0001, decay=0.00001)\n",
    "\n",
    "Nadam = keras.optimizers.Nadam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=0.00001, schedule_decay=0.0004)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"Nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import time\n",
    "import more_itertools as mit\n",
    "\n",
    "\n",
    "how_many_step = 40\n",
    "how_many_repeat = 25\n",
    "\n",
    "ix_repeat = 0\n",
    "\n",
    "\n",
    "while (ix_repeat < how_many_repeat):\n",
    "    ix_repeat += 1\n",
    "    \n",
    "    print (\"REPEAT:\", ix_repeat)\n",
    "    ix_step = 0\n",
    "    from_epi = 2\n",
    "    \n",
    "    while (ix_step < how_many_step):\n",
    "        ix_step += 1\n",
    "\n",
    "        print (\"STEP:\", ix_step)\n",
    "        \n",
    "        #print(\"relax\")\n",
    "        #time.sleep(2.5) \n",
    "        try:\n",
    "            input_array, output_array = create_data_for_supervised (\"./amicorpus/*/audio/\", 100, 100, from_epi, from_epi+3, True, 12, True, False, True)\n",
    "\n",
    "            print (np.mean(output_array))\n",
    "\n",
    "            max_len = 800 # how many frame will be taken\n",
    "            step = 800 # step size.\n",
    "\n",
    "            input_array_specified = []\n",
    "            output_array_specified = []\n",
    "\n",
    "            for i in range (0, input_array.shape[0]-max_len, step):\n",
    "                single_input_specified = (input_array[i:i+max_len,:])\n",
    "                single_output_specified = (output_array[i:i+max_len,:])\n",
    "\n",
    "                input_array_specified.append(single_input_specified)\n",
    "                output_array_specified.append(single_output_specified)\n",
    "\n",
    "            output_array_specified = np.asarray(output_array_specified)\n",
    "            input_array_specified = np.asarray(input_array_specified)\n",
    "            print (output_array_specified.shape)\n",
    "            print (input_array_specified.shape)\n",
    "\n",
    "            try:\n",
    "                model.fit(input_array_specified, output_array_specified,\n",
    "                   epochs=5,\n",
    "                   batch_size=8,\n",
    "                   validation_split = 0.2,\n",
    "                   shuffle=False)\n",
    "\n",
    "            except ValueError :\n",
    "                print (\"Pass this epoch.\")\n",
    "                pass\n",
    "            \n",
    "            model.save_weights('bilstm_cnn_crepe.h5')    \n",
    "\n",
    "            input_array = []\n",
    "            output_array = []\n",
    "        except IndexError:\n",
    "            print (\"Index Error. Pass this.\")\n",
    "            pass\n",
    "\n",
    "        from_epi += 5\n",
    "    model.save_weights(\"bilstm_cnn_crepe\" + str(ix_repeat) + \".h5\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
