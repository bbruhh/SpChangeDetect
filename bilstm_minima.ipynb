{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use 25 ms window length with 10 ms hop\n",
    "# This feature extraction is based on pyannote library\n",
    "\n",
    "f = open(\"featureplan_new.txt\", \"w\")\n",
    "f.write(\"mfcc: MFCC blockSize=400 stepSize=160 CepsNbCoeffs=19 \\n\"\n",
    "        \"mfcc_d1: MFCC blockSize=400 stepSize=160 CepsNbCoeffs=19 > Derivate DOrder=1 \\n\"\n",
    "        \"mfcc_d2: MFCC blockSize=400 stepSize=160 CepsNbCoeffs=19 > Derivate DOrder=2 \\n\"\n",
    "        \"energy_d1: Energy blockSize=400 stepSize=160 > Derivate DOrder=1 \\n\"\n",
    "        \"energy_d2: Energy blockSize=400 stepSize=160 > Derivate DOrder=2\")\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "\n",
    "def create_numpy_for_audio(audio_file, featureplan = \"\", hop=10, win_len=25, sr=16000):\n",
    "    \"\"\"This function is based on YAAFE or Librosa. It will return 2D Array which is features of audio file. \n",
    "    Also it will save the numpy array.\n",
    "    \n",
    "    Its arguments:\n",
    "    audio_file: Path of audio file, it can be wav, mp3, ogg etc.\n",
    "    featureplan: Text file which introduce which features will be extracted.(for Yaafe)\n",
    "    feature_extractor: Which library will be used to extract features.\n",
    "    hop: Hop length (we need it for Librosa)\n",
    "    win_len: Window length (we need it for Librosa)\n",
    "    sr: Sample rate for audio file. Default as 16000.\"\"\"\n",
    "    \n",
    "    if (feature_extractor==\"yaafe\"):\n",
    "        !yaafe -c $featureplan -r $sr $audio_file -p Precision=6 -p Metadata=False -n\n",
    "        filename = (audio_file.split(\"/\")[-1]).split(\".\")[0]\n",
    "\n",
    "        my_data = genfromtxt(audio_file + \".mfcc.csv\", delimiter=',')\n",
    "        my_data = np.append(my_data, genfromtxt(audio_file + \".mfcc_d1.csv\", delimiter=','), axis=1)\n",
    "        my_data = np.append(my_data, genfromtxt(audio_file + \".mfcc_d2.csv\", delimiter=','), axis=1)\n",
    "\n",
    "        my_data = np.append(my_data, np.expand_dims(genfromtxt(audio_file + \".energy_d1.csv\", delimiter=','), axis=1), axis=1)\n",
    "        my_data = np.append(my_data, np.expand_dims(genfromtxt(audio_file + \".energy_d2.csv\", delimiter=','), axis=1), axis=1)\n",
    "\n",
    "        # Previous codes creates csv file for features to load numpy array. After that, we can \n",
    "        # remove them.\n",
    "        os.remove(audio_file + \".mfcc.csv\")\n",
    "        os.remove(audio_file + \".mfcc_d1.csv\")\n",
    "        os.remove(audio_file + \".mfcc_d2.csv\")\n",
    "        os.remove(audio_file + \".energy_d1.csv\")\n",
    "        os.remove(audio_file + \".energy_d2.csv\")\n",
    "        \n",
    "        np.save(\"./yaafe_ami_storage/\" + filename, my_data)\n",
    "\n",
    "        return my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as pp\n",
    "import more_itertools as mit\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def create_data_for_supervised(root_dir, hop, win_len, from_ep=0, to_ep=0, boost_for_imbalance=False, \n",
    "                              how_much_boost=6,\n",
    "                              feature_extractor=\"pyannote\",\n",
    "                              fuzzy_label=False):\n",
    "    \n",
    "    \"\"\"If we have numpy array in the folder, it will create input and output array via this function.\n",
    "    \n",
    "    Arguments:\n",
    "    root_dir: The folder which stores numpy array.\n",
    "    hop: Hop length (we need it for Librosa).\n",
    "    win_len: Window length (we need it for Librosa).\n",
    "    from_ep: Location of first file which will be loaded into array.\n",
    "    to_ep: Location of last file which will be loaded into array.\n",
    "    boost_for_imbalance: If it is true, the number of positive labels is\n",
    "        increased artificially by labeling as positive every frame in the\n",
    "        direct  neighborhood  of  the  manually  annotated  change  point\n",
    "        and this number will be determined by how_much_boost parameter.\n",
    "    balance: if it is true, we will discard some frame which are at the \n",
    "        middle of single speaker segment.\n",
    "    feature_extractor: Which library will be used. (now, we can use Pyannote or Yaafe.)\n",
    "    fuzzy_label: If it is true, we will label the neighboorhood frames\n",
    "        with decreasing number. (for instance: if we will use 5 frame as a boost\n",
    "        middle frame will be 1, next frame will be 0.8, next one will be 0.6)\n",
    "    \"\"\"\n",
    "    \n",
    "    audios_path = glob.glob(os.path.join(root_dir, '*wav'))\n",
    "    input_array = []\n",
    "    \n",
    "    output= []\n",
    "    \n",
    "    num = 0\n",
    "    \n",
    "    for single_audio_path in audios_path:\n",
    "        \n",
    "        num += 1\n",
    "        \n",
    "        if ((num >= from_ep) and (num < to_ep)):\n",
    "            \n",
    "            change_seconds = []\n",
    "\n",
    "            filename = (single_audio_path.split(\"/\")[-1]).split(\".\")[0]\n",
    "            print (filename)\n",
    "            \n",
    "            if (feature_extractor == \"pyannote\"):\n",
    "                feature_vector = np.load(\"./pyannote-audio/tutorials/feature-extraction/AMI/\" + filename + \".Mix-Headset.npy\")\n",
    "\n",
    "            if (feature_extractor == \"yaafe\"):\n",
    "                feature_vector = np.load(\"./yaafe_ami_storage/\" + filename + \".Mix-Headset.npy\")\n",
    "\n",
    "                print (feature_vector.shape)\n",
    "\n",
    "            feature_array = np.ravel(feature_vector)\n",
    "\n",
    "            if (feature_vector is not None):\n",
    "\n",
    "                input_array.extend(feature_array)\n",
    "\n",
    "                print (single_audio_path + \" is done.\")\n",
    "\n",
    "                path_for_output = \"./txt_ami_full/\" + filename + \"_full_time.txt\"\n",
    "\n",
    "                with open(path_for_output) as f:\n",
    "                    content = f.readlines()\n",
    "\n",
    "                content = [x.strip() for x in content] \n",
    "\n",
    "                for single_change in content:\n",
    "\n",
    "                    change_seconds.append(single_change)\n",
    "\n",
    "                output_array = np.zeros(feature_vector.shape[0])\n",
    "\n",
    "                for single_change in change_seconds:\n",
    "\n",
    "                    single_change_ms = float(single_change)*1000\n",
    "\n",
    "                    which_start_hop = (single_change_ms-win_len)/hop # now we know, milisecond version of change\n",
    "                                                # which is located after which_hop paramater\n",
    "                                                # add 2 and round to up\n",
    "\n",
    "                    which_end_hop = single_change_ms/hop # round to up\n",
    "\n",
    "                    start_location = math.ceil(which_start_hop + 1)\n",
    "                    end_location = math.ceil(which_end_hop)\n",
    "\n",
    "                    if (boost_for_imbalance==False):\n",
    "\n",
    "                        output_array[start_location:end_location+1] = 1.0\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        if (fuzzy_label==False):\n",
    "                            output_array[start_location-how_much_boost:end_location+1+how_much_boost] = 1.0\n",
    "                        else:\n",
    "                            output_array[start_location:end_location+1] = 1.0\n",
    "                            for ix_label in range(1, how_much_boost):\n",
    "                                output_array[start_location-ix_label] = 1.0 - (float(ix_label)/how_much_boost)\n",
    "                                output_array[end_location+1+ix_label] = 1.0 - (float(ix_label)/how_much_boost)\n",
    "\n",
    "                output.extend(output_array)\n",
    "\n",
    "    input_array = np.reshape(input_array, (-1, 59))\n",
    "        \n",
    "    output_array = np.asarray(output)\n",
    "    output_array = np.expand_dims(output_array, axis=1)\n",
    "    \n",
    "    print(\"inputs shape: \", input_array.shape)\n",
    "\n",
    "    print(\"outputs shape: \", output_array.shape)\n",
    "\n",
    "    return (input_array, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import time\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "frame_shape = (800, 59)\n",
    "\n",
    "## Network Architecture\n",
    "\n",
    "input_frame = keras.Input(frame_shape, name='main_input')\n",
    "\n",
    "bidirectional_1 = layers.Bidirectional(layers.LSTM(64, activation=\"tanh\", return_sequences=True))(input_frame)\n",
    "bidirectional_1_drop = layers.Dropout(0.2)(bidirectional_1)\n",
    "bidirectional_2 = layers.Bidirectional(layers.LSTM(32, activation='tanh', return_sequences=True))(bidirectional_1_drop)\n",
    "\n",
    "tdistributed_1 = layers.TimeDistributed(layers.Dense(40, activation='tanh'))(bidirectional_2)\n",
    "tdistributed_1_drop = layers.Dropout(0.2)(tdistributed_1)\n",
    "tdistributed_2 = layers.TimeDistributed(layers.Dense(10, activation='tanh'))(tdistributed_1_drop)\n",
    "tdistributed_3 = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(tdistributed_2)\n",
    "\n",
    "\n",
    "# WE DO NOT NEED IT FOR TRAINING. SO DISCARD.\n",
    "## Source: https://stackoverflow.com/questions/37743574/hard-limiting-threshold-activation-function-in-tensorflow\n",
    "def step_activation(x):\n",
    "    threshold = 0.4\n",
    "    cond = tf.less(x, tf.fill(value=threshold, dims=tf.shape(x)))\n",
    "    out = tf.where(cond, tf.zeros(tf.shape(x)), tf.ones(tf.shape(x)))\n",
    "\n",
    "    return out\n",
    "\n",
    "# https://stackoverflow.com/questions/47034692/keras-set-output-of-intermediate-layer-to-0-or-1-based-on-threshold\n",
    "\n",
    "step_activation = layers.Dense(1, activation=step_activation, name='threshold_activation')(tdistributed_3)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(input_frame, tdistributed_3)\n",
    "\n",
    "rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.0001, decay=0.00001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_step = 10\n",
    "how_many_repeat = 10\n",
    "\n",
    "ix_repeat = 0\n",
    "\n",
    "\n",
    "while (ix_repeat < how_many_repeat):\n",
    "    ix_repeat += 0\n",
    "    \n",
    "    print (\"REPEAT:\", ix_repeat)\n",
    "    ix_step = 0\n",
    "    from_epi = 1\n",
    "    \n",
    "    while (ix_step < how_many_step):\n",
    "        ix_step += 1\n",
    "\n",
    "        print (\"STEP:\", ix_step)\n",
    "        \n",
    "        #print(\"relax\")\n",
    "        #time.sleep(2.5) \n",
    "    \n",
    "        try:\n",
    "            input_array, output_array = create_data_for_supervised (\"./amicorpus/*/audio/\", 10, 25, from_epi, \n",
    "                                                                    from_epi+5, True, 40, \"yaafe\", True)\n",
    "\n",
    "            print (np.mean(output_array))\n",
    "            max_len = 800 # how many frame will be taken\n",
    "            step = 800 # step size.\n",
    "\n",
    "            input_array_specified = []\n",
    "            output_array_specified = []\n",
    "\n",
    "            for i in range (0, input_array.shape[0]-max_len, step):\n",
    "                single_input_specified = (input_array[i:i+max_len,:])\n",
    "                single_output_specified = (output_array[i:i+max_len,:])\n",
    "\n",
    "                input_array_specified.append(single_input_specified)\n",
    "                output_array_specified.append(single_output_specified)\n",
    "\n",
    "            output_array_specified = np.asarray(output_array_specified)\n",
    "            input_array_specified = np.asarray(input_array_specified)\n",
    "\n",
    "\n",
    "            try:\n",
    "                model.fit(input_array_specified, output_array_specified,\n",
    "                       epochs=4,\n",
    "                       batch_size=16,\n",
    "                       validation_split=0.2,\n",
    "                       shuffle=False)\n",
    "\n",
    "            except:\n",
    "                print (\"Pass this epoch.\")\n",
    "                pass\n",
    "        except:\n",
    "            print (\"Possibly file error.\")\n",
    "            pass\n",
    "\n",
    "        # https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\n",
    "\n",
    "        model.save_weights('bilstm_weights_yaafe.h5')    \n",
    "\n",
    "        input_array = []\n",
    "        output_array = []\n",
    "\n",
    "        from_epi += 3\n",
    "    model.save_weights(\"bilstm_weights_yaafe\" + str(ix_repeat) + \".h5\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get prediction, we need to give k, 800, 59 array to system.\n",
    "# Our output is like k, 320, 1\n",
    "# We need to convert it into milisecond version\n",
    "\n",
    "import more_itertools as mit\n",
    "\n",
    "def create_prediction(filename, hop, win_len, threshold, lstm_system, featureplan, sr, feature_extractor=\"yaafe\"):\n",
    "    \n",
    "    \"\"\"\"It takes audio file and create prediction via lstm system. If output exceeds\n",
    "    threshold, we will say there is speaker change.\n",
    "    \n",
    "    Arguments:\n",
    "    filename= Which file will be considered.\n",
    "    hop: Hop length (we need it for Librosa)\n",
    "    win_len: Window length (we need it for Librosa)\n",
    "    threshold: If prediction exceed this value, we will say there is speaker change\n",
    "    lstm_system: System will create prediction\n",
    "    featureplan: Which txt will be used for yaafe feature extraction.\n",
    "    sr: Sample rate of audio input.\n",
    "    \n",
    "    Outputs:\n",
    "    prediction_array: It stores prediction value for each frame\n",
    "    prediction_array_rav: Ravel version of prediction array. We will use it.\n",
    "    prediction_array_ms = It stores which milisecond we have speaker change point.\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction_vector = []\n",
    "    \n",
    "    if (feature_extractor==\"pyannote\"):\n",
    "        feature_vector = np.load(\"./pyannote-audio/tutorials/feature-extraction/AMI/\" + filename + \".npy\")\n",
    "\n",
    "    if (feature_extractor==\"yaafe\"):\n",
    "        \n",
    "        try:\n",
    "            feature_vector = np.load(\"./yaafe_ami_storage/\" + filename + \".npy\")\n",
    "        except: \n",
    "            feature_vector = create_numpy_for_audio(audio_file=\"rad_bremen_media/\" + filename + \".wav\", \n",
    "                                                            feature_extractor=\"yaafe\", \n",
    "                                                            hop=hop, win_len=win_len, \n",
    "                                                            featureplan=featureplan, sr=sr)\n",
    "    ix_frame = 0\n",
    "    \n",
    "    \n",
    "    print (feature_vector.shape[0])\n",
    "\n",
    "    while (ix_frame+799<feature_vector.shape[0]):        \n",
    "\n",
    "        prediction = lstm_system.predict(np.expand_dims(feature_vector[ix_frame:ix_frame+800], axis=0))\n",
    "\n",
    "        prediction_vector.append(prediction)\n",
    "        ix_frame += 800\n",
    "\n",
    "    prediction_vector = np.asarray(prediction_vector)\n",
    "    print (prediction_vector.shape)\n",
    "\n",
    "    prediction_array = np.ravel(prediction_vector)\n",
    "\n",
    "\n",
    "    prediction_array_sec = []\n",
    "    prediction_array_msec = []\n",
    "    ix_frame_pred = 0\n",
    "\n",
    "    for pred in prediction_array:\n",
    "        if (pred > threshold):\n",
    "            ms_version = float(win_len + (ix_frame_pred * hop)) # milisecond version to represent end point of first embed            \n",
    "            prediction_array_msec.append(int(ms_version))\n",
    "            prediction_array_sec.append(ms_version/1000)\n",
    "\n",
    "        ix_frame_pred += 1\n",
    "\n",
    "    prediction_array_tenth_ms = np.asarray(prediction_array_msec)/10\n",
    "\n",
    "    list_cons = [list(group) for group in mit.consecutive_groups(prediction_array_tenth_ms)]\n",
    "\n",
    "    prediction_array_msec_smooth = []\n",
    "\n",
    "    for single_list_cons in list_cons:\n",
    "        prediction_array_msec_smooth.append(np.mean(single_list_cons)*0.01)\n",
    "\n",
    "    prediction_array_msec_smooth = np.asarray(prediction_array_msec_smooth)\n",
    "\n",
    "    which_turn = 0\n",
    "\n",
    "    for single_mean_s in prediction_array_msec_smooth:\n",
    "        which_turn += 1\n",
    "\n",
    "        try:\n",
    "            start_time = float(prediction_array_msec_smooth[which_turn-1])\n",
    "            end_time = float(prediction_array_msec_smooth[which_turn])\n",
    "\n",
    "            if ((start_time+0.5) > end_time):\n",
    "                mean_s[which_turn] = ((start_time+end_time) / 2)\n",
    "                mean_s = np.delete(mean_s, which_turn-1)\n",
    "                which_turn -= 1\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "                \n",
    "    # https://codereview.stackexchange.com/questions/5196/grouping-consecutive-numbers-into-ranges-in-python-3-2\n",
    "\n",
    "    np.savetxt(fname=\"./prediction_txt/\" + filename + \"_prediction.txt\", \n",
    "               X=prediction_array_msec_smooth, \n",
    "               delimiter=' ', fmt='%1.3f')\n",
    "\n",
    "    return (prediction_array, prediction_array_msec_smooth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_to_matrix (filename, type_of_text, sr, featureplan, hop=10, win_len=25, feature_extractor=\"yaafe\"):\n",
    "    \n",
    "    \n",
    "    \"\"\"It takes the reference(ground truth) text file or prediction text file (they are in second version) and \n",
    "    return output array which represent the which frames has a speaker change point.\n",
    "    \n",
    "    Arguments:\n",
    "    filename: Which file will be considered.\n",
    "    type_of_text: Is it prediction or reference file.\n",
    "    sr: Sample rate.\n",
    "    featureplan: Which txt will be used for yaafe feature extraction.\n",
    "    hop: Hop length. (for Librosa.)\n",
    "    win_len: Window length. (for Librosa)\n",
    "    feature_extractor: Which feature extractor will be used. (Now, we have 2 option as\n",
    "        Pyannote or Yaafe.)\"\"\"\n",
    "    \n",
    "    \n",
    "    if (feature_extractor==\"pyannote\"):\n",
    "        feature_vector = np.load(\"./pyannote-audio/tutorials/feature-extraction/AMI/\" + filename + \".npy\")\n",
    "        \n",
    "    if (feature_extractor==\"yaafe\"):\n",
    "        try:\n",
    "            feature_vector = np.load(\"./yaafe_ami_storage/\" + filename + \".npy\")\n",
    "        except: \n",
    "            feature_vector = create_numpy_for_audio(audio_file=\"rad_bremen_media/\" + filename + \".wav\", \n",
    "                                                            feature_extractor=\"yaafe\", hop=hop, win_len=win_len, \n",
    "                                                            featureplan=featureplan, sr=sr)\n",
    "    \n",
    "    \n",
    "    if (type_of_text == \"reference\"):\n",
    "        path_for_txt = \"./txt_ami_full/\" + filename.split(\".\")[0] + \"_full_time.txt\" \n",
    "        print (path_for_txt)\n",
    "    \n",
    "    if (type_of_text == \"prediction\"):\n",
    "        path_for_txt = \"./prediction_txt/\" + filename + \"_prediction.txt\"\n",
    "        \n",
    "    change_seconds = []\n",
    "\n",
    "\n",
    "    with open(path_for_txt) as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "    content = [x.strip() for x in content] \n",
    "    \n",
    "    for single_change in content:\n",
    "        change_seconds.append(single_change)\n",
    "    \n",
    "    output_array = np.zeros(feature_vector.shape[0])\n",
    "\n",
    "    for single_change in change_seconds:\n",
    "        \n",
    "        single_change_ms = float(single_change)*1000\n",
    "        which_start_hop = (single_change_ms-win_len)/hop # now we know, milisecond version of change\n",
    "                                    # which is located after which_hop paramater\n",
    "                                    # add 2 and round to up\n",
    "        start_location = math.ceil(which_start_hop + 1)\n",
    "\n",
    "        output_array[start_location] = 1.0\n",
    "\n",
    "    return (output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metric\n",
    "from pyannote.metrics.diarization import DiarizationPurityCoverageFMeasure\n",
    "from pyannote.metrics.segmentation import SegmentationPurity\n",
    "from pyannote.metrics.segmentation import SegmentationCoverage\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from pyannote.database import get_protocol\n",
    "from pyannote.core import Segment, Timeline, Annotation\n",
    "\n",
    "\n",
    "metric = DiarizationPurityCoverageFMeasure()\n",
    "seg_purity = SegmentationPurity()\n",
    "seg_coverage = SegmentationCoverage()\n",
    "\n",
    "def complete_evaluation_single_file(filename, hop, win_len, thres, model, sr, featureplan, feature_extractor):\n",
    "    \n",
    "    \"\"\"With this function, we can get different metrics to evaluate the system's performance.\n",
    "    \n",
    "    Arguments:\n",
    "    filename: Which file will be considered.\n",
    "    hop: Hop length.\n",
    "    win_len: Window length.\n",
    "    thres:\n",
    "    model: Which Deep Learning model will be used to predict.\n",
    "    sr: Sample rate of input.\n",
    "    featureplan:\n",
    "    overlapping: Will we use overlapping-averaging to predict output.\n",
    "    show_graph_frame: If it is true, we can see the prediction\n",
    "        and ground truth at the figure.\n",
    "    feature_extractor: Which feature extractor will be used. (currently, \n",
    "        Pyannote and Yaafe are available.)\"\"\"\n",
    "    \n",
    "    \n",
    "    prediction_array, prediction_array_msec = create_prediction(filename, hop=hop, win_len=win_len, \n",
    "                                                                threshold = thres, lstm_system=model,\n",
    "                                                                featureplan=featureplan,\n",
    "                                                                sr=sr,\n",
    "                                                                feature_extractor=feature_extractor,)\n",
    "    \n",
    "    ground_truth = txt_file_to_matrix(filename, \"reference\", sr, featureplan, \n",
    "                                      hop=hop, win_len=win_len, feature_extractor=feature_extractor)\n",
    "    \n",
    "    prediction_output_array = txt_file_to_matrix(filename, \"prediction\", sr, featureplan, \n",
    "                                                 hop=hop, win_len=win_len, feature_extractor=feature_extractor)\n",
    "    \n",
    "    path_for_reference = \"./txt_ami_full/\" + filename.split(\".\")[0] + \"_full_time.txt\"\n",
    "   \n",
    "        \n",
    "    path_for_prediction = \"./prediction_txt/\" + filename + \"_prediction.txt\"\n",
    "   \n",
    "        \n",
    "    with open(path_for_reference) as f:\n",
    "        change_seconds_ref = f.readlines()\n",
    "        \n",
    "    change_seconds_ref = [x.strip() for x in change_seconds_ref] \n",
    "    \n",
    "    change_miliseconds_ref = [int(1000*float(i)) for i in change_seconds_ref]\n",
    "    \n",
    "    x_axis = np.arange(1, len(ground_truth)+1)\n",
    "    \n",
    "    pp.rcParams['figure.figsize'] = (50.8, 10.0)\n",
    "    pp.plot(prediction_array[0:20000])\n",
    "    pp.plot(x_axis[0:20000], ground_truth[0:20000], 'x', color='black');\n",
    "    # pp.plot(x_axis, 0.95*prediction_output_array, '.', color=\"pink\");\n",
    "    \n",
    "    # pp.axhline(y=thres, color='r', linestyle='-')\n",
    "    pp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"TS3003a.Mix-Headset\", hop=10, win_len=25, thres = 0.18, \n",
    "                               model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
