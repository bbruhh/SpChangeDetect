{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOME NOTES\n",
    "\n",
    "- To improve performance, check https://svail.github.io/rnn_perf/\n",
    "\n",
    "    - For readers in first group (users of deep learning frameworks) - the main takeaway is to make layer sizes and mini-batch sizes multiples of 32, and if you're using cuBLAS, then make them multiples of 64 for best performance. If you're writing recurrent layers yourself, make sure to write them in such a way that you combine across time when possible. \n",
    "    \n",
    "- Try to hyperparameter optimization: http://maxpumperla.com/hyperas/\n",
    "\n",
    "- SincNet can be helpful. https://github.com/mravanelli/SincNet/\n",
    "\n",
    "- Check the transformer. https://jalammar.github.io/illustrated-transformer/ https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "\n",
    "- Autokeras. https://github.com/jhfjhfj1/autokeras\n",
    "\n",
    "- Should we use FeedForward? https://www.offconvex.org/2018/07/27/approximating-recurrent/\n",
    "\n",
    "- TCN https://github.com/philipperemy/keras-tcn\n",
    "\n",
    "- Another method for speaker change detection https://github.com/philipperemy/speaker-change-detection\n",
    "\n",
    "- Rare sound event detection: https://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Lim_204.pdf\n",
    "\n",
    "- We can augment data via [Pydub](https://github.com/jiaaro/pydub). https://towardsdatascience.com/tensorflow-speech-recognition-challenge-solution-outline-9c42dbd219c9\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are trying to reproduce [the paper](https://pdfs.semanticscholar.org/edff/b62b32ffcc2b5cc846e26375cb300fac9ecc.pdf) for speaker change detection\n",
    "\n",
    "**TODO**\n",
    "\n",
    "- Use AMI Corpus with feature extraction with pyannote.\n",
    "\n",
    "## Review\n",
    "\n",
    "**Sequence Labelling** \n",
    "\n",
    "They think this task as a binary classification. Thus, they label changing frame as a **1** and non-changing frame as a **0**. So that, they use the _binary cross-entropy loss function_.\n",
    "\n",
    "**Network Architecture**\n",
    "- 2 Bi-LSTM\n",
    "    - 64 and 32 outputs respectively.\n",
    "- Multi Layer Perceptron\n",
    "    - 3 Fully Connected Feedforward Layers\n",
    "        - 40, 20, 1 dimensional respectively.\n",
    "    - Tanh activation for first 2 layer\n",
    "    - Sigmoid activation for last layer\n",
    "    \n",
    "**Feature Extraction**\n",
    "- \"35-dimensional acoustic features are extracted every 16ms on a 32ms window using [Yaafe toolkit](http://yaafe.sourceforge.net).\"\n",
    "    - 11 Mel-Frequency Cepstral Coefficients (MFCC), \n",
    "    - Their first and second derivatives,\n",
    "    - First and second derivatives of the energy.\n",
    "\n",
    "**Class Imbalance**\n",
    "\n",
    "- _\"The number of positive labels isincreased artificially by labeling as positive every frame in the direct neighborhood of the manually annotated change point.\"_\n",
    "- A positive neighborhood of 100ms (50ms on both sides) is used around each change point, to partially solve the class imbalance problem.\n",
    "\n",
    "**Subsequences**\n",
    "\n",
    "- _\"The long audio sequences are split into short fixed-length overlapping sequences.\"_\n",
    "\n",
    "**Prediction**\n",
    "\n",
    "- _\"Finally, local score maxima exceeding a pre-determined threshold Î¸ are marked as speaker change points.\"_\n",
    "\n",
    "**Training**\n",
    "\n",
    "- Subsequences for training are 3.2s long with a step of 800ms (i.e. two adjacent sequences overlap by 75%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Codebase**\n",
    "\n",
    "- For Optimizer: _SGD, momentum=0.9, nesterov=True_\n",
    "\n",
    "- Architecture:\n",
    "       name: StackedRNN\n",
    "       params:\n",
    "         rnn: LSTM\n",
    "         recurrent: [32, 20]\n",
    "         bidirectional: True\n",
    "         linear: [40, 10]\n",
    "         \n",
    "-  create final classification layer (with log-softmax activation)\n",
    "    - NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE OUR YAAFE IMPLEMENTATION TO EXTRACT NUMPY ARRAY INTO ONE FOLDER\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from numpy import genfromtxt\n",
    "\n",
    "\n",
    "def create_numpy_for_wav(featureplan, audio_file, sr):\n",
    "    !yaafe -c $featureplan -r $sr $audio_file -p Precision=6 -p Metadata=False -n\n",
    "    filename = (audio_file.split(\"/\")[-1]).split(\".wav\")[0]\n",
    "    \n",
    "    my_data = genfromtxt(audio_file + \".mfcc.csv\", delimiter=',')\n",
    "    my_data = np.append(my_data, genfromtxt(audio_file + \".mfcc_d1.csv\", delimiter=','), axis=1)\n",
    "    my_data = np.append(my_data, genfromtxt(audio_file + \".mfcc_d2.csv\", delimiter=','), axis=1)\n",
    "\n",
    "    my_data = np.append(my_data, np.expand_dims(genfromtxt(audio_file + \".energy_d1.csv\", delimiter=','), axis=1), axis=1)\n",
    "    my_data = np.append(my_data, np.expand_dims(genfromtxt(audio_file + \".energy_d2.csv\", delimiter=','), axis=1), axis=1)\n",
    "    \n",
    "    os.remove(audio_file + \".mfcc.csv\")\n",
    "    os.remove(audio_file + \".mfcc_d1.csv\")\n",
    "    os.remove(audio_file + \".mfcc_d2.csv\")\n",
    "    os.remove(audio_file + \".energy_d1.csv\")\n",
    "    os.remove(audio_file + \".energy_d2.csv\")\n",
    "\n",
    "    np.save(\"./yaafe_ami_storage/\" + filename, my_data)\n",
    "    print (filename + \" is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "We will use Yaafe Toolkit. (To see all available features, you can use _!yaafe -l_) To learn how we can do that, start with http://yaafe.github.io/Yaafe/manual/quickstart.html#quick-start-using-yaafe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can view a description of each feature (or output format) with the -d option:\n",
    "\n",
    "# !yaafe -d MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yaafe -d Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine blockSize and stepSize. \n",
    "\n",
    "If we have 16kHz audio signal(in AMI, we have 16kHz), for 32 ms block, we need 16x32, For the stepsize as 16 ms, we need 16x16 size.\n",
    "\n",
    "We need these features:\n",
    "\n",
    "- mfcc: MFCC blockSize=512 stepSize=256 CepsNbCoeffs=11\n",
    "- mfcc_d1: MFCC blockSize=512 stepSize=256 CepsNbCoeffs=11 > Derivate DOrder=1\n",
    "- mfcc_d2: MFCC blockSize=512 stepSize=256 CepsNbCoeffs=11 > Derivate DOrder=2\n",
    "- energy_d1: Energy blockSize=512 stepSize=256  > Derivate DOrder=1\n",
    "- energy_d2: Energy blockSize=512 stepSize=256  > Derivate DOrder=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract all of these, we will use [this technique](http://yaafe.github.io/Yaafe/manual/quickstart.html#extract-several-features). Shortly, we will write all these features into single text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"featureplan.txt\", \"w\")\n",
    "f.write(\"mfcc: MFCC blockSize=512 stepSize=256 CepsNbCoeffs=11 \\n\"\n",
    "        \"mfcc_d1: MFCC blockSize=512 stepSize=256 CepsNbCoeffs=11 > Derivate DOrder=1 \\n\"\n",
    "        \"mfcc_d2: MFCC blockSize=512 stepSize=256 CepsNbCoeffs=11 > Derivate DOrder=2 \\n\"\n",
    "        \"energy_d1: Energy blockSize=512 stepSize=256  > Derivate DOrder=1 \\n\"\n",
    "        \"energy_d2: Energy blockSize=512 stepSize=256  > Derivate DOrder=2\")\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"featureplan_new.txt\", \"w\")\n",
    "f.write(\"mfcc: MFCC blockSize=400 stepSize=160 CepsNbCoeffs=19 \\n\"\n",
    "        \"mfcc_d1: MFCC blockSize=400 stepSize=160 CepsNbCoeffs=19 > Derivate DOrder=1 \\n\"\n",
    "        \"mfcc_d2: MFCC blockSize=400 stepSize=160 CepsNbCoeffs=19 > Derivate DOrder=2 \\n\"\n",
    "        \"energy_d1: Energy blockSize=400 stepSize=160 > Derivate DOrder=1 \\n\"\n",
    "        \"energy_d2: Energy blockSize=400 stepSize=160 > Derivate DOrder=2\")\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"featureplan_new_48000.txt\", \"w\")\n",
    "f.write(\"mfcc: MFCC blockSize=1200 stepSize=480 CepsNbCoeffs=19 \\n\"\n",
    "        \"mfcc_d1: MFCC blockSize=1200 stepSize=480 CepsNbCoeffs=19 > Derivate DOrder=1 \\n\"\n",
    "        \"mfcc_d2: MFCC blockSize=1200 stepSize=480 CepsNbCoeffs=19 > Derivate DOrder=2 \\n\"\n",
    "        \"energy_d1: Energy blockSize=1200 stepSize=480 > Derivate DOrder=1 \\n\"\n",
    "        \"energy_d2: Energy blockSize=1200 stepSize=480 > Derivate DOrder=2\")\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"featureplan_new_44100.txt\", \"w\")\n",
    "f.write(\"mfcc: MFCC blockSize=1102.5 stepSize=441 CepsNbCoeffs=19 \\n\"\n",
    "        \"mfcc_d1: MFCC blockSize=1102.5 stepSize=441 CepsNbCoeffs=19 > Derivate DOrder=1 \\n\"\n",
    "        \"mfcc_d2: MFCC blockSize=1102.5 stepSize=441 CepsNbCoeffs=19 > Derivate DOrder=2 \\n\"\n",
    "        \"energy_d1: Energy blockSize=1102.5 stepSize=441 > Derivate DOrder=1 \\n\"\n",
    "        \"energy_d2: Energy blockSize=1102.5 stepSize=441 > Derivate DOrder=2\")\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yaafe -c \"featureplan_new.txt\" -r 16000 \"ES2009a.Mix-Headset.wav\" -p Precision=6 -p Metadata=False -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "\n",
    "def create_numpy_for_audio(audio_file, featureplan = \"\", feature_extractor=\"yaafe\", hop=10, win_len=25, sr=16000):\n",
    "    ## This function is based on YAAFE and librosa. Its arguments:\n",
    "    # featureplan: Text file which introduce which features will be extracted. (we need it for YAAFE)\n",
    "    # audio_file: Path of audio file, it can be wav, mp3, ogg etc.\n",
    "    # feature_extractor: Which library will be used to extract features\n",
    "    # hop: Hop length (we need it for Librosa)\n",
    "    # win_len: Window length (we need it for Librosa)\n",
    "    \n",
    "    ## It will return 2D Array which is features of audio file. Also\n",
    "    # it will save the numpy array.\n",
    "    \n",
    "    if (feature_extractor==\"yaafe\"):\n",
    "        !yaafe -c $featureplan -r $sr $audio_file -p Precision=6 -p Metadata=False -n\n",
    "        filename = (audio_file.split(\"/\")[-1]).split(\".\")[0]\n",
    "\n",
    "        my_data = genfromtxt(audio_file + \".mfcc.csv\", delimiter=',')\n",
    "        my_data = np.append(my_data, genfromtxt(audio_file + \".mfcc_d1.csv\", delimiter=','), axis=1)\n",
    "        my_data = np.append(my_data, genfromtxt(audio_file + \".mfcc_d2.csv\", delimiter=','), axis=1)\n",
    "\n",
    "        my_data = np.append(my_data, np.expand_dims(genfromtxt(audio_file + \".energy_d1.csv\", delimiter=','), axis=1), axis=1)\n",
    "        my_data = np.append(my_data, np.expand_dims(genfromtxt(audio_file + \".energy_d2.csv\", delimiter=','), axis=1), axis=1)\n",
    "\n",
    "        # Previous codes creates csv file for features to load numpy array. After that, we can \n",
    "        # remove them.\n",
    "        os.remove(audio_file + \".mfcc.csv\")\n",
    "        os.remove(audio_file + \".mfcc_d1.csv\")\n",
    "        os.remove(audio_file + \".mfcc_d2.csv\")\n",
    "        os.remove(audio_file + \".energy_d1.csv\")\n",
    "        os.remove(audio_file + \".energy_d2.csv\")\n",
    "        # np.save(\"./yaafe_ami_storage/\" + filename, my_data)\n",
    "\n",
    "\n",
    "        return my_data\n",
    "    \n",
    "    if (feature_extractor==\"librosa\"):\n",
    "        audio, sr = librosa.load(filename)\n",
    "        # https://github.com/librosa/librosa/issues/584\n",
    "        mfccs = librosa.feature.mfcc(audio, sr, n_mfcc=11, hop_length=int(float(hop/1000)*sr), n_fft=int(float(win_len/1000)*sr))\n",
    "        mfccs_d1 = librosa.feature.delta(mfccs)\n",
    "        mfccs_d2 = librosa.feature.delta(mfccs, order=2)\n",
    "        energy = librosa.feature.rmse(y=audio, hop_length=int(float(hop/1000)*sr), frame_length=int(float(win_len/1000)*sr))\n",
    "        energy_d1 = librosa.feature.delta(energy)\n",
    "        energy_d2 = librosa.feature.delta(energy, order=2)\n",
    "        print (mfccs.shape)\n",
    "        print (mfccs_d1.shape)\n",
    "        print (mfccs_d2.shape)\n",
    "        print (energy_d1.shape)\n",
    "        print (energy_d2.shape)\n",
    "\n",
    "        my_data = np.vstack((mfccs, mfccs_d1, mfccs_d2, energy_d1, energy_d2))\n",
    "        np.save(\"./librosa_ami_storage/\" + filename, my_data)\n",
    "        # line_mfccs = np.ravel(mfccs, order='F')\n",
    "        return my_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand, how normalization and delta features affect performance https://ieeexplore.ieee.org/document/5711789/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as pp\n",
    "import more_itertools as mit\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def create_data_for_supervised(root_dir, hop, win_len, from_ep=0, to_ep=0, boost_for_imbalance=False, how_much_boost=6, balance=False, \n",
    "                              feature_extractor=\"pyannote\",\n",
    "                              overlapping=False):\n",
    "    \n",
    "    ## If we have numpy array in the folder, we will create input and output array via this function. Its arguments:\n",
    "    # root_dir: The folder which stores numpy array.\n",
    "    # hop: Hop length (we need it for Librosa)\n",
    "    # win_len: Window length (we need it for Librosa)\n",
    "    # from_ep: Location of first file which will be loaded into array\n",
    "    # to_ep: Location of last file which will be loaded into array\n",
    "    # boost_for_imbalance: If it is true, the number of positive labels is\n",
    "    # increased artificially by labeling as positive every frame in the\n",
    "    # direct  neighborhood  of  the  manually  annotated  change  point\n",
    "    # and this number will be determined by how_much_boost parameter\n",
    "    # balance: if it is true, we will discard some frame which are at the \n",
    "    # middle of single speaker segment.\n",
    "    \n",
    "    \n",
    "    all_audio_paths = glob.glob(os.path.join(root_dir, '*wav'))\n",
    "    matrix_of_all_audio = []\n",
    "    \n",
    "    output_all_array = []\n",
    "    num = 0\n",
    "    \n",
    "    for single_audio_path in all_audio_paths:\n",
    "        num += 1\n",
    "        \n",
    "        if ((num >= from_ep) and (num < to_ep)):\n",
    "            \n",
    "            end_time_array_second = []\n",
    "\n",
    "            filename = (single_audio_path.split(\"/\")[-1]).split(\".\")[0]\n",
    "            \n",
    "            try:\n",
    "                if (feature_extractor == \"pyannote\"):\n",
    "                    matrix_of_single_audio = np.load(\"/home/herdogan/Desktop/SpChangeDetect/pyannote-audio/tutorials/feature-extraction/AMI/\" + filename + \".Mix-Headset.npy\")\n",
    "                    \n",
    "                if (feature_extractor == \"yaafe\"):\n",
    "                    matrix_of_single_audio = np.load(\"/home/herdogan/Desktop/SpChangeDetect/yaafe_ami_storage/\" + filename + \".Mix-Headset.npy\")\n",
    "                    \n",
    "                    print (matrix_of_single_audio.shape)\n",
    "                    \n",
    "                array_of_single_audio = np.ravel(matrix_of_single_audio)\n",
    "\n",
    "                if (matrix_of_single_audio is not None):\n",
    "\n",
    "                    matrix_of_all_audio.extend(array_of_single_audio)\n",
    "                    print (single_audio_path + \" is done.\")\n",
    "\n",
    "                    main_set = \"./txt_files/\" + filename + \"_end_time.txt\"# FILENAME PATH for TXT\n",
    "\n",
    "                    with open(main_set) as f:\n",
    "                        content = f.readlines()\n",
    "\n",
    "                    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "\n",
    "\n",
    "                    # need to open text file\n",
    "                    # after that, point the end point of speaker\n",
    "                    # add 1 to point of speaker, add 0 otherwise\n",
    "                    # time is in second format at the txt file\n",
    "                    content = [x.strip() for x in content] \n",
    "\n",
    "                    for single_line in content:\n",
    "\n",
    "                        end_time_array_second.append(single_line)\n",
    "\n",
    "                        # we use following method to get milisecond version\n",
    "                        # float(win_len + ((offset+100) * hop)) \n",
    "                        # we need to inversion of that\n",
    "                    # print (end_time_array_second)\n",
    "\n",
    "                    output_array = np.zeros(matrix_of_single_audio.shape[0])\n",
    "\n",
    "                    for end_time in end_time_array_second:\n",
    "                        end_time_ms = float(end_time)*1000\n",
    "                        \n",
    "                        which_start_hop = (end_time_ms-win_len)/hop # now we know, milisecond version of change\n",
    "                                                    # which is located after which_hop paramater\n",
    "                                                    # add 2 and round to up\n",
    "                        which_end_hop = end_time_ms/hop # round to up\n",
    "\n",
    "                        start_location = math.ceil(which_start_hop + 1)\n",
    "                        end_location = math.ceil(which_end_hop)\n",
    "\n",
    "                        # print (\"s:\", start_location)\n",
    "                        # print (\"e:\", end_location)\n",
    "                        if (boost_for_imbalance==False):\n",
    "                            output_array[start_location:end_location+1] = 1.0\n",
    "\n",
    "                        else:\n",
    "                            output_array[start_location-how_much_boost:end_location+1+how_much_boost] = 1.0\n",
    "                    output_all_array.extend(output_array)\n",
    "            except:\n",
    "                print (\"Pass this file...\")\n",
    "                pass\n",
    "            # print (output_array)\n",
    "            # print (output_array.mean())\n",
    "            # ar = np.arange(matrix_of_single_audio.shape[1]) # just as an example array\n",
    "            # pp.plot(ar, output_array, 'x')\n",
    "            # pp.show()\n",
    "    \n",
    "    # if (overlapping):\n",
    "        ### IMPLEMENT IT\n",
    "            \n",
    "    audio_array = np.asarray(matrix_of_all_audio)\n",
    "    audio_array = np.reshape(matrix_of_all_audio, (-1, 59))\n",
    "    \n",
    "    input_array = audio_array\n",
    "    \n",
    "    output_all_array = np.asarray(output_all_array)\n",
    "    output_all_array = np.expand_dims(output_all_array, axis=1)\n",
    "    \n",
    "    if (balance == True):\n",
    "            loc_zeros = np.where(output_all_array == 0)[0]\n",
    "            list_cons = [list(group) for group in mit.consecutive_groups(loc_zeros)]\n",
    "            for single_list_con in list_cons:\n",
    "                if (len(single_list_con) > 80):\n",
    "                    first_zero_loc = single_list_con[0]\n",
    "                    last_zero_loc = single_list_con[-1]\n",
    "                    output_all_array[first_zero_loc+20:last_zero_loc-20] = 2\n",
    "            loc_twos = np.where(output_all_array == 2)[0]\n",
    "            list_cons = [list(group) for group in mit.consecutive_groups(loc_twos)]\n",
    "            output_all_array = output_all_array.squeeze(axis=1)\n",
    "            # print (output_all_array.shape)\n",
    "            input_array = input_array[output_all_array != 2, :]\n",
    "            # [idx==0,:]\n",
    "            output_all_array = output_all_array[output_all_array != 2]   \n",
    "            output_all_array = np.expand_dims(output_all_array, axis=1)\n",
    "    \n",
    "    print(\"inputs shape: \", input_array.shape)\n",
    "\n",
    "    print(\"outputs shape: \", output_all_array.shape)\n",
    "\n",
    "    return (input_array, output_all_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "frame_shape = (320, 59)\n",
    "\n",
    "## Network Architecture\n",
    "\n",
    "input_frame = keras.Input(frame_shape, name='main_input')\n",
    "\n",
    "bidirectional_1 = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(input_frame)\n",
    "bidirectional_2 = layers.Bidirectional(layers.LSTM(32, activation='tanh', return_sequences=True))(bidirectional_1)\n",
    "\n",
    "tdistributed_1 = layers.TimeDistributed(layers.Dense(40, activation='tanh'))(bidirectional_2)\n",
    "tdistributed_2 = layers.TimeDistributed(layers.Dense(20, activation='tanh'))(tdistributed_1)\n",
    "tdistributed_3 = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(tdistributed_2)\n",
    "\n",
    "\n",
    "# WE DO NOT NEED IT FOR TRAINING. SO DISCARD.\n",
    "## Source: https://stackoverflow.com/questions/37743574/hard-limiting-threshold-activation-function-in-tensorflow\n",
    "def step_activation(x):\n",
    "    threshold = 0.4\n",
    "    cond = tf.less(x, tf.fill(value=threshold, dims=tf.shape(x)))\n",
    "    out = tf.where(cond, tf.zeros(tf.shape(x)), tf.ones(tf.shape(x)))\n",
    "\n",
    "    return out\n",
    "\n",
    "# https://stackoverflow.com/questions/47034692/keras-set-output-of-intermediate-layer-to-0-or-1-based-on-threshold\n",
    "\n",
    "step_activation = layers.Dense(1, activation=step_activation, name='threshold_activation')(tdistributed_3)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(input_frame, tdistributed_3)\n",
    "\n",
    "rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.0001, decay=0.00001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"bilstm_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import time\n",
    "\n",
    "\n",
    "how_many_step = 100\n",
    "how_many_repeat = 15\n",
    "\n",
    "ix_repeat = 0\n",
    "\n",
    "\n",
    "while (ix_repeat < how_many_repeat):\n",
    "    ix_repeat += 1\n",
    "    \n",
    "    print (\"REPEAT:\", ix_repeat)\n",
    "    ix_step = 0\n",
    "    from_epi = 0\n",
    "    \n",
    "    while (ix_step < how_many_step):\n",
    "        ix_step += 1\n",
    "\n",
    "        print (\"STEP:\", ix_step)\n",
    "        \n",
    "        #print(\"relax\")\n",
    "        #time.sleep(2.5) \n",
    "\n",
    "        input_array, output_array = create_data_for_supervised (\"./amicorpus/*/audio/\", 10, 25, from_epi, from_epi+1, True, 6, False, feature_extractor=\"yaafe\")\n",
    "\n",
    "        max_len = 320 # how many frame will be taken\n",
    "        step = 320 # step size.\n",
    "\n",
    "        input_array_specified = []\n",
    "        output_array_specified = []\n",
    "\n",
    "        for i in range (0, input_array.shape[0]-max_len, step):\n",
    "            single_input_specified = (input_array[i:i+max_len,:])\n",
    "            single_output_specified = (output_array[i:i+max_len,:])\n",
    "\n",
    "            input_array_specified.append(single_input_specified)\n",
    "            output_array_specified.append(single_output_specified)\n",
    "\n",
    "        output_array_specified = np.asarray(output_array_specified)\n",
    "        input_array_specified = np.asarray(input_array_specified)\n",
    "\n",
    "        try:\n",
    "\n",
    "            model.fit(input_array_specified, output_array_specified,\n",
    "                   epochs=1,\n",
    "                   batch_size=16,\n",
    "                   shuffle=True)\n",
    "\n",
    "        except:\n",
    "            print (\"Pass this epoch.\")\n",
    "            pass\n",
    "\n",
    "        # https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\n",
    "\n",
    "        model.save_weights('bilstm_weights.h5')    \n",
    "\n",
    "        input_array = []\n",
    "        output_array = []\n",
    "\n",
    "        from_epi += 1\n",
    "    model.save_weights(\"bilstm_weights\" + str(ix_repeat) + \".h5\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "frame_shape = (800, 59)\n",
    "\n",
    "## Network Architecture\n",
    "\n",
    "input_frame = keras.Input(frame_shape, name='main_input')\n",
    "\n",
    "bidirectional_1 = layers.Bidirectional(layers.LSTM(64, activation=\"tanh\", return_sequences=True))(input_frame)\n",
    "bidirectional_2 = layers.Bidirectional(layers.LSTM(32, activation='tanh', return_sequences=True))(bidirectional_1)\n",
    "\n",
    "# compute importance for each step\n",
    "attention = layers.Dense(1, activation='tanh')(bidirectional_2)\n",
    "attention = layers.Flatten()(attention)\n",
    "attention = layers.Activation('softmax')(attention)\n",
    "attention = layers.RepeatVector(64)(attention)\n",
    "attention = layers.Permute([2, 1])(attention)\n",
    "\n",
    "multiplied = layers.Multiply()([bidirectional_2, attention])\n",
    "sent_representation = layers.Dense(512)(multiplied)\n",
    "\n",
    "tdistributed_1 = layers.TimeDistributed(layers.Dense(40, activation='tanh'))(sent_representation)\n",
    "tdistributed_2 = layers.TimeDistributed(layers.Dense(10, activation='tanh'))(tdistributed_1)\n",
    "tdistributed_3 = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(tdistributed_2)\n",
    "\n",
    "\n",
    "# WE DO NOT NEED IT FOR TRAINING. SO DISCARD.\n",
    "## Source: https://stackoverflow.com/questions/37743574/hard-limiting-threshold-activation-function-in-tensorflow\n",
    "def step_activation(x):\n",
    "    threshold = 0.4\n",
    "    cond = tf.less(x, tf.fill(value=threshold, dims=tf.shape(x)))\n",
    "    out = tf.where(cond, tf.zeros(tf.shape(x)), tf.ones(tf.shape(x)))\n",
    "\n",
    "    return out\n",
    "\n",
    "# https://stackoverflow.com/questions/47034692/keras-set-output-of-intermediate-layer-to-0-or-1-based-on-threshold\n",
    "\n",
    "step_activation = layers.Dense(1, activation=step_activation, name='threshold_activation')(tdistributed_3)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(input_frame, tdistributed_3)\n",
    "\n",
    "rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.0001, decay=0.00001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.load_weights(\"bilstm_weights_yaafe_att10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import time\n",
    "\n",
    "\n",
    "how_many_step = 100\n",
    "how_many_repeat = 30\n",
    "\n",
    "ix_repeat = 0\n",
    "\n",
    "\n",
    "while (ix_repeat < how_many_repeat):\n",
    "    ix_repeat += 1\n",
    "    \n",
    "    print (\"REPEAT:\", ix_repeat)\n",
    "    ix_step = 0\n",
    "    from_epi = 0\n",
    "    \n",
    "    while (ix_step < how_many_step):\n",
    "        ix_step += 1\n",
    "\n",
    "        print (\"STEP:\", ix_step)\n",
    "        \n",
    "        #print(\"relax\")\n",
    "        #time.sleep(2.5) \n",
    "\n",
    "        input_array, output_array = create_data_for_supervised (\"./amicorpus/*/audio/\", 10, 25, from_epi, from_epi+1, True, 5, False, \"yaafe\" )\n",
    "\n",
    "        max_len = 800 # how many frame will be taken\n",
    "        step = 800 # step size.\n",
    "\n",
    "        input_array_specified = []\n",
    "        output_array_specified = []\n",
    "\n",
    "        for i in range (0, input_array.shape[0]-max_len, step):\n",
    "            single_input_specified = (input_array[i:i+max_len,:])\n",
    "            single_output_specified = (output_array[i:i+max_len,:])\n",
    "\n",
    "            input_array_specified.append(single_input_specified)\n",
    "            output_array_specified.append(single_output_specified)\n",
    "\n",
    "        output_array_specified = np.asarray(output_array_specified)\n",
    "        input_array_specified = np.asarray(input_array_specified)\n",
    "\n",
    "        try:\n",
    "\n",
    "            model.fit(input_array_specified, output_array_specified,\n",
    "                   epochs=1,\n",
    "                   batch_size=16,\n",
    "                   shuffle=True)\n",
    "\n",
    "        except:\n",
    "            print (\"Pass this epoch.\")\n",
    "            pass\n",
    "\n",
    "        # https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\n",
    "\n",
    "        model.save_weights('bilstm_weights_yaafe_att.h5')    \n",
    "\n",
    "        input_array = []\n",
    "        output_array = []\n",
    "\n",
    "        from_epi += 1\n",
    "    model.save_weights(\"bilstm_weights_yaafe_att\" + str(ix_repeat) + \".h5\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "frame_shape = (800, 59)\n",
    "\n",
    "## Network Architecture\n",
    "# https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "input_frame = keras.Input(frame_shape, name='main_input')\n",
    "\n",
    "conv1 = layers.Conv1D(800, 3, activation=\"relu\", padding=\"same\")(input_frame)\n",
    "conv1_drop = layers.Dropout(0.25)(maxpool1)\n",
    "\n",
    "conv2 = layers.Conv1D(800, 3, activation=\"relu\", padding=\"same\")(conv1)\n",
    "conv2_drop = layers.Dropout(0.25)(conv2)\n",
    "\n",
    "\n",
    "# flatten1 = layers.Flatten()(conv1)\n",
    "\n",
    "# keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "\n",
    "bidirectional_1 = layers.Bidirectional(layers.LSTM(32, return_sequences=True, kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))(conv2_drop)\n",
    "bidirectional_1_drop = layers.Dropout(0.25)(bidirectional_1)\n",
    "bidirectional_2 = layers.Bidirectional(layers.LSTM(16, activation='tanh', return_sequences=True,  kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))(bidirectional_1_drop)\n",
    "\n",
    "# compute importance for each step\n",
    "attention = layers.Dense(1, activation='tanh')(bidirectional_2)\n",
    "attention = layers.Flatten()(attention)\n",
    "attention = layers.Activation('softmax')(attention)\n",
    "attention = layers.RepeatVector(32)(attention)\n",
    "attention = layers.Permute([2, 1])(attention)\n",
    "\n",
    "multiplied = layers.Multiply()([bidirectional_2, attention])\n",
    "sent_representation = layers.Dense(256)(multiplied)\n",
    "\n",
    "tdistributed_1 = layers.TimeDistributed(layers.Dense(20, activation='tanh', kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))(sent_representation)\n",
    "tdistributed_1_drop = layers.Dropout(0.25)(tdistributed_1)\n",
    "tdistributed_2 = layers.TimeDistributed(layers.Dense(5, activation='tanh', kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))(tdistributed_1_drop)\n",
    "tdistributed_2_drop = layers.Dropout(0.25)(tdistributed_2)\n",
    "tdistributed_3 = layers.TimeDistributed(layers.Dense(1, activation='sigmoid', kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros'))(tdistributed_2_drop)\n",
    "\n",
    "\n",
    "# WE DO NOT NEED IT FOR TRAINING. SO DISCARD.\n",
    "## Source: https://stackoverflow.com/questions/37743574/hard-limiting-threshold-activation-function-in-tensorflow\n",
    "def step_activation(x):\n",
    "    threshold = 0.4\n",
    "    cond = tf.less(x, tf.fill(value=threshold, dims=tf.shape(x)))\n",
    "    out = tf.where(cond, tf.zeros(tf.shape(x)), tf.ones(tf.shape(x)))\n",
    "\n",
    "    return out\n",
    "\n",
    "# https://stackoverflow.com/questions/47034692/keras-set-output-of-intermediate-layer-to-0-or-1-based-on-threshold\n",
    "\n",
    "step_activation = layers.Dense(1, activation=step_activation, name='threshold_activation')(tdistributed_3)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(input_frame, tdistributed_3)\n",
    "\n",
    "rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.0001, decay=0.00001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 800, 59)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 800, 800)     142400      main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 800, 800)     1920800     conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 800, 800)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_42 (Bidirectional (None, 800, 64)      213248      dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 800, 64)      0           bidirectional_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_43 (Bidirectional (None, 800, 32)      10368       dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_97 (Dense)                (None, 800, 1)       33          bidirectional_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 800)          0           dense_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 800)          0           flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_21 (RepeatVector) (None, 32, 800)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute_21 (Permute)            (None, 800, 32)      0           repeat_vector_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_21 (Multiply)          (None, 800, 32)      0           bidirectional_43[0][0]           \n",
      "                                                                 permute_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_98 (Dense)                (None, 800, 256)     8448        multiply_21[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_58 (TimeDistri (None, 800, 20)      5140        dense_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 800, 20)      0           time_distributed_58[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_59 (TimeDistri (None, 800, 5)       105         dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 800, 5)       0           time_distributed_59[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_60 (TimeDistri (None, 800, 1)       6           dropout_29[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,300,548\n",
      "Trainable params: 2,300,548\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPEAT: 1\n",
      "STEP: 1\n",
      "(211317, 59)\n",
      "./amicorpus/IS1007c/audio/IS1007c.Mix-Headset.wav is done.\n",
      "inputs shape:  (211317, 59)\n",
      "outputs shape:  (211317, 1)\n",
      "Epoch 1/4\n",
      "264/264 [==============================] - 87s 330ms/step - loss: 0.6660\n",
      "Epoch 2/4\n",
      "264/264 [==============================] - 79s 298ms/step - loss: 0.5615\n",
      "Epoch 3/4\n",
      "264/264 [==============================] - 77s 292ms/step - loss: 0.4839\n",
      "Epoch 4/4\n",
      "264/264 [==============================] - 80s 302ms/step - loss: 0.4375\n",
      "STEP: 2\n",
      "(138419, 59)\n",
      "./amicorpus/ES2016a/audio/ES2016a.Mix-Headset.wav is done.\n",
      "inputs shape:  (138419, 59)\n",
      "outputs shape:  (138419, 1)\n",
      "Epoch 1/4\n",
      "173/173 [==============================] - 58s 333ms/step - loss: 0.4053\n",
      "Epoch 2/4\n",
      "173/173 [==============================] - 64s 370ms/step - loss: 0.3848\n",
      "Epoch 3/4\n",
      "173/173 [==============================] - 60s 346ms/step - loss: 0.3670\n",
      "Epoch 4/4\n",
      "173/173 [==============================] - 53s 307ms/step - loss: 0.3502\n",
      "STEP: 3\n",
      "(226384, 59)\n",
      "./amicorpus/IS1004c/audio/IS1004c.Mix-Headset.wav is done.\n",
      "inputs shape:  (226384, 59)\n",
      "outputs shape:  (226384, 1)\n",
      "Epoch 1/4\n",
      "282/282 [==============================] - 85s 301ms/step - loss: 0.3414\n",
      "Epoch 2/4\n",
      "282/282 [==============================] - 90s 319ms/step - loss: 0.3207\n",
      "Epoch 3/4\n",
      "282/282 [==============================] - 96s 342ms/step - loss: 0.3010\n",
      "Epoch 4/4\n",
      "282/282 [==============================] - 98s 346ms/step - loss: 0.2838\n",
      "STEP: 4\n",
      "(240300, 59)\n",
      "./amicorpus/TS3005c/audio/TS3005c.Mix-Headset.wav is done.\n",
      "inputs shape:  (240300, 59)\n",
      "outputs shape:  (240300, 1)\n",
      "Epoch 1/4\n",
      "300/300 [==============================] - 104s 347ms/step - loss: 0.2751\n",
      "Epoch 2/4\n",
      "300/300 [==============================] - 97s 323ms/step - loss: 0.2611\n",
      "Epoch 3/4\n",
      "300/300 [==============================] - 102s 340ms/step - loss: 0.2491\n",
      "Epoch 4/4\n",
      "300/300 [==============================] - 111s 369ms/step - loss: 0.2380\n",
      "STEP: 5\n",
      "(187634, 59)\n",
      "./amicorpus/IS1003c/audio/IS1003c.Mix-Headset.wav is done.\n",
      "inputs shape:  (187634, 59)\n",
      "outputs shape:  (187634, 1)\n",
      "Epoch 1/4\n",
      "234/234 [==============================] - 79s 339ms/step - loss: 0.2296\n",
      "Epoch 2/4\n",
      "234/234 [==============================] - 70s 301ms/step - loss: 0.2219\n",
      "Epoch 3/4\n",
      "234/234 [==============================] - 67s 284ms/step - loss: 0.2162\n",
      "Epoch 4/4\n",
      "234/234 [==============================] - 69s 293ms/step - loss: 0.2109\n",
      "STEP: 6\n",
      "(309999, 59)\n",
      "./amicorpus/EN2009c/audio/EN2009c.Mix-Headset.wav is done.\n",
      "inputs shape:  (309999, 59)\n",
      "outputs shape:  (309999, 1)\n",
      "Epoch 1/4\n",
      "352/387 [==========================>...] - ETA: 10s - loss: 0.1777"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import time\n",
    "\n",
    "\n",
    "how_many_step = 100\n",
    "how_many_repeat = 30\n",
    "\n",
    "ix_repeat = 0\n",
    "\n",
    "\n",
    "while (ix_repeat < how_many_repeat):\n",
    "    ix_repeat += 1\n",
    "    \n",
    "    print (\"REPEAT:\", ix_repeat)\n",
    "    ix_step = 0\n",
    "    from_epi = 2\n",
    "    \n",
    "    while (ix_step < how_many_step):\n",
    "        ix_step += 1\n",
    "\n",
    "        print (\"STEP:\", ix_step)\n",
    "        \n",
    "        #print(\"relax\")\n",
    "        #time.sleep(2.5) \n",
    "\n",
    "        input_array, output_array = create_data_for_supervised (\"./amicorpus/*/audio/\", 10, 25, from_epi, from_epi+1, True, 6, False, \"yaafe\" )\n",
    "\n",
    "        max_len = 800 # how many frame will be taken\n",
    "        step = 800 # step size.\n",
    "\n",
    "        input_array_specified = []\n",
    "        output_array_specified = []\n",
    "\n",
    "        for i in range (0, input_array.shape[0]-max_len, step):\n",
    "            single_input_specified = (input_array[i:i+max_len,:])\n",
    "            single_output_specified = (output_array[i:i+max_len,:])\n",
    "\n",
    "            input_array_specified.append(single_input_specified)\n",
    "            output_array_specified.append(single_output_specified)\n",
    "\n",
    "        output_array_specified = np.asarray(output_array_specified)\n",
    "        input_array_specified = np.asarray(input_array_specified)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            model.fit(input_array_specified, output_array_specified,\n",
    "               epochs=4,\n",
    "               batch_size=16,\n",
    "               shuffle=False)\n",
    "\n",
    "        except ValueError:\n",
    "            print (\"Pass this epoch.\")\n",
    "            pass\n",
    "\n",
    "        # https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\n",
    "\n",
    "        model.save_weights('bilstm_cnn_yaafe_att.h5')    \n",
    "\n",
    "        input_array = []\n",
    "        output_array = []\n",
    "\n",
    "        from_epi += 1\n",
    "    model.save_weights(\"bilstm_cnn_yaafe_att\" + str(ix_repeat) + \".h5\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get prediction, we need to give k, 800, 59 array to system.\n",
    "# Our output is like k, 320, 1\n",
    "# We need to convert it into milisecond version\n",
    "\n",
    "import more_itertools as mit\n",
    "\n",
    "\n",
    "def create_prediction(filename, hop, win_len, threshold, lstm_system, featureplan, sr, overlapping = False, feature_extractor=\"yaafe\"):\n",
    "    ## It takes audio file and create prediction via lstm system. If output exceeds\n",
    "    # threshold, we will say there is speaker change. Its arguments:\n",
    "    # filename= Which file will be considered.\n",
    "    # hop: Hop length (we need it for Librosa)\n",
    "    # win_len: Window length (we need it for Librosa)\n",
    "    # threshold: If prediction exceed this value, we will say there is speaker change\n",
    "    # lstm_system: System will create prediction\n",
    "    # featureplan: Wthich txt will be used for yaafe\n",
    "    # sr: Signal rate of audio inout\n",
    "    \n",
    "    ## Function crate prediction text file in second version. Also returns:\n",
    "    # prediction_array: It stores prediction value for each frame\n",
    "    # prediction_array_rav: Ravel version of prediction array. We will use it.\n",
    "    # prediction_array_ms = It stores which milisecond we have speaker change point.\n",
    "    \n",
    "    \n",
    "    prediction_array = []\n",
    "    if (feature_extractor==\"pyannote\"):\n",
    "        matrix_of_single_audio = np.load(\"/home/herdogan/Desktop/SpChangeDetect/pyannote-audio/tutorials/feature-extraction/AMI/\" + filename + \".npy\")\n",
    "\n",
    "    if (feature_extractor==\"yaafe\"):\n",
    "        try:\n",
    "            matrix_of_single_audio = np.load(\"/home/herdogan/Desktop/SpChangeDetect/yaafe_ami_storage/\" + filename + \".npy\")\n",
    "        except: \n",
    "            matrix_of_single_audio = create_numpy_for_audio(audio_file=\"df_media/\" + filename + \".wav\", feature_extractor=\"yaafe\", hop=hop, win_len=win_len, featureplan=featureplan, sr=sr)\n",
    "    \n",
    "   \n",
    "    ix_frame = 0\n",
    "    \n",
    "    if (overlapping):\n",
    "        while ((ix_frame+798)<matrix_of_single_audio.shape[0]):        \n",
    "            # print (matrix_of_single_audio.shape)\n",
    "            # print (np.expand_dims(matrix_of_single_audio[ix_frame:ix_frame+800], axis=0).shape)\n",
    "            prediction = lstm_system.predict(np.expand_dims(matrix_of_single_audio[ix_frame:ix_frame+800], axis=0))\n",
    "            prediction = prediction.squeeze(axis=2)\n",
    "            prediction = prediction.squeeze(axis=0)\n",
    "\n",
    "            prediction_array.append(prediction)\n",
    "            # print (prediction.shape)\n",
    "            ix_frame += 200\n",
    "            \n",
    "        prediction_array = np.asarray(prediction_array)\n",
    "        print (prediction_array.shape)\n",
    "        prediction_array_rav = np.ravel(prediction_array)\n",
    "\n",
    "\n",
    "        prediction_array_sec = []\n",
    "        prediction_array_msec = []\n",
    "        prediction_array_rav_aver = []\n",
    "        ix_frame_pred = 0\n",
    "\n",
    "\n",
    "        total_prediction = len(prediction_array_rav)\n",
    "        print (total_prediction)\n",
    "\n",
    "        prediction_array_rav_aver[0:200] = prediction_array_rav[0:200]\n",
    "        prediction_array_rav_aver[200:400] = (prediction_array_rav[200:400]+prediction_array_rav[800:1000]) * 0.5\n",
    "        prediction_array_rav_aver[400:600] = (prediction_array_rav[400:600]+prediction_array_rav[1000:1200]+prediction_array_rav[1600:1800]) * 0.33\n",
    "        \n",
    "        ix_frame = 600\n",
    "        count = 0\n",
    "        while ((ix_frame+798)<total_prediction):        \n",
    "\n",
    "            next_frame = ix_frame + (count * 600) \n",
    "            try:\n",
    "                prediction_array_rav_aver[ix_frame:ix_frame+200] = (prediction_array_rav[next_frame:next_frame+200]+prediction_array_rav[next_frame+600:next_frame+800]+\n",
    "                                                               prediction_array_rav[next_frame+1200:next_frame+1400]+prediction_array_rav[next_frame+1800:next_frame+2000]) * 0.25\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # print (prediction.shape)\n",
    "            ix_frame += 200\n",
    "            count += 1\n",
    "\n",
    "\n",
    "        prediction_array_rav = np.asarray(prediction_array_rav_aver)\n",
    "\n",
    "        for pred in prediction_array_rav:\n",
    "\n",
    "            if (pred > threshold):\n",
    "                ms_version = float(win_len + (ix_frame_pred * hop)) # milisecond version to represent end point of first embed            \n",
    "                prediction_array_msec.append(int(ms_version))\n",
    "                prediction_array_sec.append(ms_version/1000)\n",
    "\n",
    "            ix_frame_pred += 1\n",
    "\n",
    "\n",
    "        prediction_array_smooth = []\n",
    "        for pred in prediction_array_msec:\n",
    "            if (pred-hop not in prediction_array_msec):\n",
    "                prediction_array_smooth.append(pred*0.001)\n",
    "\n",
    "\n",
    "        prediction_array_tenth_ms = np.asarray(prediction_array_msec)/10\n",
    "\n",
    "        list_cons = [list(group) for group in mit.consecutive_groups(prediction_array_tenth_ms)]\n",
    "\n",
    "        mean_s = []\n",
    "\n",
    "        for single_list_cons in list_cons:\n",
    "            # print (np.mean(single_list_cons))\n",
    "            mean_s.append(np.mean(single_list_cons)*0.01)\n",
    "\n",
    "        mean_s = np.asarray(mean_s)\n",
    "\n",
    "        which_turn = 0\n",
    "\n",
    "        for single_mean_s in mean_s:\n",
    "            which_turn += 1\n",
    "\n",
    "            try:\n",
    "                start_time = float(mean_s[which_turn-1])\n",
    "                end_time = float(mean_s[which_turn])\n",
    "\n",
    "                if ((start_time+0.5) > end_time):\n",
    "                    # print (\"was here\")\n",
    "                    mean_s[which_turn] = ((start_time+end_time) / 2)\n",
    "                    mean_s = np.delete(mean_s, which_turn-1)\n",
    "                    which_turn -= 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        while (ix_frame+798<matrix_of_single_audio.shape[0]):        \n",
    "            # print (matrix_of_single_audio.shape)\n",
    "            # print (np.expand_dims(matrix_of_single_audio[ix_frame:ix_frame+800], axis=0).shape)\n",
    "            prediction = lstm_system.predict(np.expand_dims(matrix_of_single_audio[ix_frame:ix_frame+800], axis=0))\n",
    "            prediction = prediction.squeeze(axis=2)\n",
    "            prediction = prediction.squeeze(axis=0)\n",
    "\n",
    "            prediction_array.append(prediction)\n",
    "            # print (prediction.shape)\n",
    "            ix_frame += 800\n",
    "        \n",
    "        prediction_array = np.asarray(prediction_array)\n",
    "        print (prediction_array.shape)\n",
    "\n",
    "        prediction_array_rav = np.ravel(prediction_array)\n",
    "\n",
    "\n",
    "        prediction_array_sec = []\n",
    "        prediction_array_msec = []\n",
    "        ix_frame_pred = 0\n",
    "\n",
    "        for pred in prediction_array_rav:\n",
    "            if (pred > threshold):\n",
    "                ms_version = float(win_len + (ix_frame_pred * hop)) # milisecond version to represent end point of first embed            \n",
    "                prediction_array_msec.append(int(ms_version))\n",
    "                prediction_array_sec.append(ms_version/1000)\n",
    "\n",
    "            ix_frame_pred += 1\n",
    "\n",
    "\n",
    "        prediction_array_smooth = []\n",
    "        for pred in prediction_array_msec:\n",
    "            if (pred-hop not in prediction_array_msec):\n",
    "                prediction_array_smooth.append(pred*0.001)\n",
    "\n",
    "\n",
    "        prediction_array_tenth_ms = np.asarray(prediction_array_msec)/10\n",
    "\n",
    "        list_cons = [list(group) for group in mit.consecutive_groups(prediction_array_tenth_ms)]\n",
    "\n",
    "        mean_s = []\n",
    "\n",
    "        for single_list_cons in list_cons:\n",
    "            # print (np.mean(single_list_cons))\n",
    "            mean_s.append(np.mean(single_list_cons)*0.01)\n",
    "\n",
    "        mean_s = np.asarray(mean_s)\n",
    "\n",
    "        which_turn = 0\n",
    "\n",
    "        for single_mean_s in mean_s:\n",
    "            which_turn += 1\n",
    "\n",
    "            try:\n",
    "                start_time = float(mean_s[which_turn-1])\n",
    "                end_time = float(mean_s[which_turn])\n",
    "\n",
    "                if ((start_time+0.5) > end_time):\n",
    "                    # print (\"was here\")\n",
    "                    mean_s[which_turn] = ((start_time+end_time) / 2)\n",
    "                    mean_s = np.delete(mean_s, which_turn-1)\n",
    "                    which_turn -= 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        \n",
    "                \n",
    "    # https://codereview.stackexchange.com/questions/5196/grouping-consecutive-numbers-into-ranges-in-python-3-2\n",
    "\n",
    "    np.savetxt(fname=\"./prediction_txt/\" + filename + \"_prediction.txt\", X=mean_s, \n",
    "               delimiter=' ', fmt='%1.3f')\n",
    "\n",
    "    return (prediction_array, prediction_array_rav, mean_s)\n",
    "\n",
    "def txt_file_to_matrix (filename,  type_of_text, sr, featureplan, hop=10, win_len=25, feature_extractor=\"yaafe\"):\n",
    "    ## It takes the reference end time text file or prediction text file (they are in second version) and \n",
    "    # return output array which represent the which frames has a speaker change point.\n",
    "    # filename= Which file will be considered.\n",
    "    # hop: Hop length (we need it for Librosa)\n",
    "    # win_len: Window length (we need it for Librosa)\n",
    "    \n",
    "    if (feature_extractor==\"pyannote\"):\n",
    "        matrix_of_single_audio = np.load(\"/home/herdogan/Desktop/SpChangeDetect/pyannote-audio/tutorials/feature-extraction/AMI/\" + filename + \".npy\")\n",
    "        \n",
    "    if (feature_extractor==\"yaafe\"):\n",
    "        try:\n",
    "            matrix_of_single_audio = np.load(\"/home/herdogan/Desktop/SpChangeDetect/yaafe_ami_storage/\" + filename + \".npy\")\n",
    "        except: \n",
    "            matrix_of_single_audio = create_numpy_for_audio(audio_file=\"df_media/\" + filename + \".wav\", feature_extractor=\"yaafe\", hop=hop, win_len=win_len, featureplan=featureplan, sr=sr)\n",
    "    \n",
    "    \n",
    "    if (type_of_text == \"reference\"):\n",
    "        main_set = \"./txt_files/\" + filename.split(\".\")[0] + \"_end_time.txt\"# FILENAME PATH for TXT\n",
    "    \n",
    "    \n",
    "    if (type_of_text == \"prediction\"):\n",
    "        main_set = \"./prediction_txt/\" + filename + \"_prediction.txt\"# FILENAME PATH for TXT\n",
    "\n",
    "    end_time_array_second = []\n",
    "\n",
    "\n",
    "    with open(main_set) as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "    content = [x.strip() for x in content] \n",
    "\n",
    "    for single_line in content:\n",
    "\n",
    "        end_time_array_second.append(single_line)\n",
    "\n",
    "    output_array = np.zeros(matrix_of_single_audio.shape[0])\n",
    "\n",
    "    for end_time in end_time_array_second:\n",
    "        end_time_ms = float(end_time)*1000\n",
    "        which_start_hop = (end_time_ms-win_len)/hop # now we know, milisecond version of change\n",
    "                                    # which is located after which_hop paramater\n",
    "                                    # add 2 and round to up\n",
    "        which_end_hop = end_time_ms/hop # round to up\n",
    "\n",
    "        start_location = math.ceil(which_start_hop + 1)\n",
    "        end_location = math.ceil(which_end_hop)\n",
    "\n",
    "        # print (\"s:\", start_location)\n",
    "        # print (\"e:\", end_location)\n",
    "        output_array[start_location:end_location+1] = 1.0\n",
    "\n",
    "    return (output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to balance our dataset to get better result. For that, we will crop the segment which include 1 speaker and its duration is long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "pp.rcParams['figure.figsize'] = (50.8, 10.0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metric\n",
    "from pyannote.metrics.diarization import DiarizationPurityCoverageFMeasure\n",
    "from pyannote.metrics.segmentation import SegmentationPurity\n",
    "from pyannote.metrics.segmentation import SegmentationCoverage\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from pyannote.database import get_protocol\n",
    "\n",
    "from pyannote.core import Segment, Timeline, Annotation\n",
    "\n",
    "\n",
    "metric = DiarizationPurityCoverageFMeasure()\n",
    "seg_purity = SegmentationPurity()\n",
    "seg_coverage = SegmentationCoverage()\n",
    "\n",
    "def complete_evaluation_single_file(filename, hop, win_len, thres, model, sr, featureplan, overlapping = False, show_graph_frame=True, feature_extractor=\"yaafe\"):\n",
    "    \n",
    "    prediction_array, prediction_array_rav, prediction_array_msec = create_prediction(filename, \n",
    "                                                                                      hop=hop, win_len=win_len, \n",
    "                                                                                      threshold = thres, lstm_system=model,\n",
    "                                                                                      feature_extractor=feature_extractor, sr=sr,\n",
    "                                                                                      featureplan=featureplan,\n",
    "                                                                                      overlapping=overlapping)\n",
    "    \n",
    "    ground_truth = txt_file_to_matrix(filename, \"reference\", sr, featureplan, hop=hop, win_len=win_len, feature_extractor=feature_extractor)\n",
    "    prediction_output_array = txt_file_to_matrix(filename, \"prediction\", sr, featureplan, hop=hop, win_len=win_len, feature_extractor=feature_extractor)\n",
    "    \n",
    "    end_time_file = \"./txt_files/\" + filename.split(\".\")[0] + \"_end_time.txt\"# FILENAME PATH for TXT\n",
    "   \n",
    "        \n",
    "    pred_file = \"./prediction_txt/\" + filename + \"_prediction.txt\"\n",
    "   \n",
    "        \n",
    "    with open(end_time_file) as f:\n",
    "        content_end_time = f.readlines()\n",
    "    content_end_time = [x.strip() for x in content_end_time] \n",
    "    content_end_time = [int(1000*float(i)) for i in content_end_time]\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    x_axis = np.arange(1, len(ground_truth)+1)\n",
    "    \n",
    "    # pp.plot(ground_truth)\n",
    "    # pp.title(\"Ground Truth of \" + filename)\n",
    "    # pp.show()\n",
    "    \n",
    "    if (show_graph_frame):\n",
    "    \n",
    "        pp.rcParams['figure.figsize'] = (50.8, 10.0)\n",
    "\n",
    "        pp.plot(prediction_array_rav[0:30000])\n",
    "        pp.plot(x_axis[0:30000], ground_truth[0:30000], 'x', color='black');\n",
    "        # pp.plot(x[0:10000], bb[200000:210000], '.', color='pink');\n",
    "        pp.plot(x_axis[0:30000], 0.95*prediction_output_array[0:30000], '.', color=\"pink\");\n",
    "\n",
    "        pp.axhline(y=thres, color='r', linestyle='-')\n",
    "        pp.show()\n",
    "\n",
    "\n",
    "    hypothesis = Annotation()\n",
    "\n",
    "    which_turn = 0\n",
    "    \n",
    "    with open(pred_file) as f:\n",
    "        content_pred = f.readlines()\n",
    "        \n",
    "    content = [x.strip() for x in content_pred] \n",
    "        \n",
    "    for single_line in content:\n",
    "        try:\n",
    "            which_turn += 1\n",
    "            start_time = content[which_turn-1]\n",
    "            end_time = content[which_turn]\n",
    "            # print (\"id: \", which_turn, \" start: \", start_time, \" end: \", end_time)\n",
    "            hypothesis[Segment(float(start_time), float(end_time))]=\"a\"\n",
    "        except:\n",
    "            pass\n",
    "   \n",
    "    content_pred = [x.strip() for x in content_pred] \n",
    "    content_pred = [int(1000*float(i)) for i in content_pred]\n",
    "    \n",
    "    \n",
    "    total_end_time_num = len(content_end_time)\n",
    "    total_guess_num = len(content_pred)\n",
    "    correct_guess = 0\n",
    "\n",
    "    for single_end_time in content_end_time:\n",
    "        single_guess_mem = False \n",
    "        for single_pred in content_pred:\n",
    "            if (single_pred in range(single_end_time-500, single_end_time+500)):\n",
    "                correct_guess += 1\n",
    "                break\n",
    "            \n",
    "    print (\"Correst Guess: \", correct_guess)    \n",
    "    print (\"Total End Time: \", total_end_time_num)    \n",
    "    print (\"Total Guess: \", total_guess_num)   \n",
    "\n",
    "    \n",
    "    precision = correct_guess / float(total_guess_num)\n",
    "    recall = correct_guess / float(total_end_time_num)\n",
    "    try:\n",
    "        f1 =  2 * (precision*recall) / (precision+recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = 0\n",
    "    \n",
    "    print (\"precison: \", precision)\n",
    "    print (\"recall: \", recall)\n",
    "    print (\"f1: \", f1)\n",
    "    \n",
    "    purity = 0\n",
    "    coverage = 0\n",
    "    \n",
    "    try:\n",
    "        protocol = get_protocol('AMI.SpeakerDiarization.MixHeadset')\n",
    "\n",
    "\n",
    "        for i in protocol.train():\n",
    "            if (i[\"uri\"] == filename):\n",
    "                 reference = i['annotation']\n",
    "\n",
    "        for i in protocol.test():\n",
    "            if (i[\"uri\"] == filename):             \n",
    "                reference = i['annotation']\n",
    "\n",
    "        for i in protocol.development():\n",
    "            if (i[\"uri\"] == filename):             \n",
    "                reference = i['annotation']\n",
    "\n",
    "        metric = SegmentationPurity()\n",
    "        purity = metric(reference, hypothesis)\n",
    "        print (\"Segmentation Purity: \", purity)\n",
    "\n",
    "        metric = SegmentationCoverage()\n",
    "        coverage = metric(reference, hypothesis)\n",
    "        print (\"Segmentation Coverage: \", coverage)\n",
    "\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    return (precision, recall, f1, purity, coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"Wie_ist_nachhaltiger_Tourismus_moeglich_2017-09-16\", hop=10, win_len=25, thres = 0.15, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"IB4001.Mix-Headset\", hop=10, win_len=25, thres = 0.18, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"IB4001.Mix-Headset\", hop=10, win_len=25, thres = 0.3, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"TS3003c.Mix-Headset\", hop=10, win_len=25, thres = 0.18, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"TS3003c.Mix-Headset\", hop=10, win_len=25, thres = 0.18, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(749, 800)\n",
      "599200\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3657.6x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correst Guess:  0\n",
      "Total End Time:  309\n",
      "Total Guess:  0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1af133e2fb55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m complete_evaluation_single_file(filename=\"TS3003a.Mix-Headset\", hop=10, win_len=25, thres = 0.18, model=model, feature_extractor=\"yaafe\",\n\u001b[1;32m      2\u001b[0m                                \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureplan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"featureplan_new.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                overlapping=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-b4c47a40e658>\u001b[0m in \u001b[0;36mcomplete_evaluation_single_file\u001b[0;34m(filename, hop, win_len, thres, model, sr, featureplan, overlapping, show_graph_frame, feature_extractor)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_guess\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_guess_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_guess\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_end_time_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "complete_evaluation_single_file(filename=\"TS3003a.Mix-Headset\", hop=10, win_len=25, thres = 0.18, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"TS3012c.Mix-Headset\", hop=10, win_len=25, thres = 0.18, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"TS3010a.Mix-Headset\", hop=10, win_len=25, thres = 0.18, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"TS3010a.Mix-Headset\", hop=10, win_len=25, thres = 0.18, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"TS3010a.Mix-Headset\", hop=10, win_len=25, thres = 0.18, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"EN2001b.Mix-Headset\", hop=10, win_len=25, thres = 0.15, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"EN2001b.Mix-Headset\", hop=10, win_len=25, thres = 0.15, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"EN2001b.Mix-Headset\", hop=10, win_len=25, thres = 0.15, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"EN2001b.Mix-Headset\", hop=10, win_len=25, thres = 0.15, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"EN2001b.Mix-Headset\", hop=10, win_len=25, thres = 0.15, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"EN2001b.Mix-Headset\", hop=10, win_len=25, thres = 0.15, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"EN2001b.Mix-Headset\", hop=10, win_len=25, thres = 0.15, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"Lauschbuben_2015-03-06\", hop=10, win_len=25, thres = 0.3, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_evaluation_single_file(filename=\"Diverser_nicht_duemmer-Die_Unis_muessen_sich_auf_andere_Studenten_einstellen\", hop=10, win_len=25, thres = 0.3, model=model, feature_extractor=\"yaafe\",\n",
    "                               sr=16000, featureplan = \"featureplan_new.txt\",\n",
    "                               overlapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO GET ALL RESULT FROM ALL CORPUS\n",
    "\n",
    "precision_array = []\n",
    "recall_array = []\n",
    "f1_array = []\n",
    "purity_array = []\n",
    "coverage_array = []\n",
    "\n",
    "corpus = os.listdir(\"/home/herdogan/Desktop/SpChangeDetect/df_media/\")\n",
    "\n",
    "for file in corpus:\n",
    "    try:\n",
    "        filename = file.split(\".\")[0]\n",
    "        print (filename)\n",
    "        \n",
    "        precision, recall, f1, _, _ = complete_evaluation_single_file(filename, hop=10, win_len=25, feature_extractor=\"yaafe\", model=model, show_graph_frame=False, thres=0.18, sr=16000, featureplan=\"featureplan_new.txt\")\n",
    "        precision_array.append(precision)\n",
    "        recall_array.append(recall)\n",
    "        f1_array.append(f1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print (filename + \" is not in the directory\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO GET ALL RESULT FROM ALL CORPUS\n",
    "\n",
    "precision_array = []\n",
    "recall_array = []\n",
    "f1_array = []\n",
    "purity_array = []\n",
    "coverage_array = []\n",
    "\n",
    "corpus = os.listdir(\"/home/herdogan/Desktop/SpChangeDetect/df_media/\")\n",
    "\n",
    "for file in corpus:\n",
    "    try:\n",
    "        filename = file.split(\".\")[0]\n",
    "        print (filename)\n",
    "        \n",
    "        precision, recall, f1, _, _ = complete_evaluation_single_file(filename, hop=10, win_len=25, feature_extractor=\"yaafe\", model=model, show_graph_frame=False, thres=0.16, sr=16000, featureplan=\"featureplan_new.txt\", overlapping=True)\n",
    "        precision_array.append(precision)\n",
    "        recall_array.append(recall)\n",
    "        f1_array.append(f1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print (filename + \" is not in the directory\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO GET ALL RESULT FROM ALL CORPUS\n",
    "\n",
    "precision_array = []\n",
    "recall_array = []\n",
    "f1_array = []\n",
    "purity_array = []\n",
    "coverage_array = []\n",
    "\n",
    "corpus = os.listdir(\"/home/herdogan/Desktop/SpChangeDetect/txt_ami_full/\")\n",
    "\n",
    "for file in corpus:\n",
    "    try:\n",
    "        filename = (file.split(\".\")[0]).split(\"_\")[0]\n",
    "        filename = filename + \".Mix-Headset\"\n",
    "        print (filename)\n",
    "        \n",
    "        precision, recall, f1, purity, coverage = complete_evaluation_single_file(filename, hop=10, win_len=25, feature_extractor=\"yaafe\", model=model, show_graph_frame=False, thres=0.168, sr=16000, featureplan=\"featureplan_new.txt\", overlapping=True)\n",
    "        precision_array.append(precision)\n",
    "        recall_array.append(recall)\n",
    "        f1_array.append(f1)\n",
    "        purity_array.append(purity)\n",
    "        coverage_array.append(coverage)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print (filename + \" is not in the directory\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = os.listdir(\"/home/herdogan/Desktop/SpChangeDetect/amicorpus/*/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(precision_array))\n",
    "print(np.mean(recall_array))\n",
    "print(np.mean(f1_array))\n",
    "print(np.mean(purity_array))\n",
    "print(np.mean(coverage_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = os.listdir(\"/home/herdogan/Desktop/SpChangeDetect/df_media/\")\n",
    "\n",
    "for file in corpus:\n",
    "    print ((file.split(\".\")[0])[:-4])\n",
    "    new_file = ((file.split(\".\")[0])[:-4]) + \".wav\" \n",
    "    !sox $file -r 16000 $new_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Subsequences with Label\n",
    "\n",
    "At that point, we should create training and test data with their label. Also, we can use directly [pyannote.metrics](https://github.com/pyannote/pyannote-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Architecture\n",
    "\n",
    "We can directly upload the model's architecture from the .yml file which is provided by writer.\n",
    "\n",
    "However, I want to directly write all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author's .yml files\n",
    "\n",
    "!wget https://raw.githubusercontent.com/yinruiqing/change_detection/master/model/architecture.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to model\n",
    "\n",
    "from keras.models import model_from_yaml\n",
    "yaml_file = open('architecture.yml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "model = model_from_yaml(loaded_model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "frame_shape = (320, 35)\n",
    "\n",
    "## Network Architecture\n",
    "\n",
    "input_frame = keras.Input(frame_shape, name='main_input')\n",
    "\n",
    "bidirectional_1 = layers.Bidirectional(layers.LSTM(32, return_sequences=True))(input_frame)\n",
    "bidirectional_2 = layers.Bidirectional(layers.LSTM(20, activation='tanh', return_sequences=True))(bidirectional_1)\n",
    "\n",
    "tdistributed_1 = layers.TimeDistributed(layers.Dense(40, activation='tanh'))(bidirectional_2)\n",
    "tdistributed_2 = layers.TimeDistributed(layers.Dense(10, activation='tanh'))(tdistributed_1)\n",
    "tdistributed_3 = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(tdistributed_2)\n",
    "\n",
    "\n",
    "# WE DO NOT NEED IT FOR TRAINING. SO DISCARD.\n",
    "## Source: https://stackoverflow.com/questions/37743574/hard-limiting-threshold-activation-function-in-tensorflow\n",
    "def step_activation(x):\n",
    "    threshold = 0.4\n",
    "    cond = tf.less(x, tf.fill(value=threshold, dims=tf.shape(x)))\n",
    "    out = tf.where(cond, tf.zeros(tf.shape(x)), tf.ones(tf.shape(x)))\n",
    "\n",
    "    return out\n",
    "\n",
    "# https://stackoverflow.com/questions/47034692/keras-set-output-of-intermediate-layer-to-0-or-1-based-on-threshold\n",
    "\n",
    "step_activation = layers.Dense(1, activation=step_activation, name='threshold_activation')(tdistributed_3)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(input_frame, tdistributed_3)\n",
    "\n",
    "rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save our model\n",
    "\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To look our model\n",
    "\n",
    "!cat model.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
